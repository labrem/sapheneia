{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"collapsed_sections":["60bGlOUe7NbL","WRfN9YHuqEjt","I5IG-66AqAvU","7gAJYADn7cr9","evocIFoO9BlE","RoGwr0WcWxR5","1PH8miFH7rzt","fvHgLMX-7xb8","4uReKoG49elK"],"machine_shape":"hm","authorship_tag":"ABX9TyNo0nllEA3OzhOlYdJPOH2g"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"markdown","source":["https://gemini.google.com/app/eb62c40f8924dab1"],"metadata":{"id":"vXZeS40_CYY7"}},{"cell_type":"code","source":["import os\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/',force_remount=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OfffPEakjVdw","executionInfo":{"status":"ok","timestamp":1749730100386,"user_tz":240,"elapsed":14490,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"0e428dcb-5a0a-413b-8ae3-0dc03eb3ddd5"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}]},{"cell_type":"markdown","source":["# TinyLlama/TinyLlama-1.1B-Chat-v1.0 (quantized)\n","\n"],"metadata":{"id":"60bGlOUe7NbL"}},{"cell_type":"markdown","source":["https://huggingface.co/TinyLlama/TinyLlama-1.1B-Chat-v1.0"],"metadata":{"id":"SYG4A451CLKi"}},{"cell_type":"markdown","source":["## setup"],"metadata":{"id":"WRfN9YHuqEjt"}},{"cell_type":"code","source":["# Install remaining required libraries\n","!pip install -U \"optimum[onnxruntime]\" transformers accelerate datasets fsspec huggingface_hub"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Km-6fLpWkQq_","executionInfo":{"status":"ok","timestamp":1749658356296,"user_tz":240,"elapsed":3020,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"dd6f8082-a336-42e9-8325-1df04a3d38cd"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.51.3)\n","Collecting transformers\n","  Using cached transformers-4.52.4-py3-none-any.whl.metadata (38 kB)\n","Requirement already satisfied: accelerate in /usr/local/lib/python3.11/dist-packages (1.7.0)\n","Requirement already satisfied: datasets in /usr/local/lib/python3.11/dist-packages (3.6.0)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (2025.3.0)\n","Collecting fsspec\n","  Using cached fsspec-2025.5.1-py3-none-any.whl.metadata (11 kB)\n","Requirement already satisfied: huggingface_hub in /usr/local/lib/python3.11/dist-packages (0.32.6)\n","Requirement already satisfied: optimum[onnxruntime] in /usr/local/lib/python3.11/dist-packages (1.25.3)\n","Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (2.6.0+cu124)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (24.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (2.0.2)\n","Requirement already satisfied: onnx in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (1.18.0)\n","Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (5.29.5)\n","Requirement already satisfied: onnxruntime>=1.11.0 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (1.22.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from accelerate) (5.9.5)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (18.1.0)\n","Requirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets) (3.5.0)\n","Requirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.11/dist-packages (from datasets) (0.70.15)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface_hub) (1.1.2)\n","Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (3.11.15)\n","Requirement already satisfied: coloredlogs in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (15.0.1)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (25.2.10)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (1.13.1)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.6)\n","Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n","Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n","Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (9.1.0.70)\n","Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.5.8)\n","Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.2.1.3)\n","Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (10.3.5.147)\n","Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (11.6.1.9)\n","Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.3.1.170)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n","Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.11.0->optimum[onnxruntime]) (1.3.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets) (2025.2)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<=2025.3.0,>=2023.1.0->datasets) (1.20.0)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n","Requirement already satisfied: humanfriendly>=9.1 in /usr/local/lib/python3.11/dist-packages (from coloredlogs->onnxruntime>=1.11.0->optimum[onnxruntime]) (10.0)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (3.0.2)\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, pipeline\n","from optimum.onnxruntime import ORTModelForCausalLM, ORTQuantizer\n","from optimum.onnxruntime import AutoQuantizationConfig"],"metadata":{"id":"hqr3FRaqi5G1"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define path for unquantized model\n","source_path = '/content/drive/MyDrive/dev/.models/TinyLlama-1.1B-Chat-v1.0/'\n","\n","# Verify that the model folders exist\n","print(\"Checking for model directories...\")\n","if os.path.exists(source_path):\n","    print(f\"Found base directory: {source_path}\")\n","    print(\"Contents:\", os.listdir(source_path))\n","else:\n","    print(f\"ERROR: The directory '{source_path}' was not found.\")\n","    print(\"Please check the folder name and its location in your Google Drive.\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JogJVdJ2jP4O","executionInfo":{"status":"ok","timestamp":1749658394477,"user_tz":240,"elapsed":5,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"1b595d73-cfeb-4b2c-a0f9-b68a5f088548"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Checking for model directories...\n","Found base directory: /content/drive/MyDrive/dev/.models/TinyLlama-1.1B-Chat-v1.0/\n","Contents: ['.git', 'README.md', 'config.json', '.gitattributes', 'eval_results.json', 'generation_config.json', 'special_tokens_map.json', 'tokenizer.json', 'tokenizer_config.json', 'tokenizer.model', 'model.safetensors']\n"]}]},{"cell_type":"code","source":["# Create save_path only if it does not already exist\n","save_path = '/content/drive/MyDrive/dev/.models/TinyLlama-1.1B-Chat-v1.0_quantized_onnx/'\n","os.makedirs(save_path, exist_ok=True)\n","\n","# Add tokenizer model directory inside save_path\n","tokenizer_path = save_path+'tokenizer/'\n","os.makedirs(tokenizer_path, exist_ok=True)\n","\n","# Add CPU optimized quantized model directory inside save_path\n","model_cpu = save_path+'model_cpu/'\n","os.makedirs(model_cpu, exist_ok=True)\n","\n","# Add GPU optimized quantized model directory inside save_path\n","model_gpu = save_path+'model_gpu/'\n","os.makedirs(model_gpu, exist_ok=True)"],"metadata":{"id":"DBbPGy8D6HcQ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the tokenizer\n","tokenizer = AutoTokenizer.from_pretrained(source_path)"],"metadata":{"id":"hXwWRf4rjP6S"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["tokenizer.save_pretrained(tokenizer_path)\n","\n","print(f\"\\nTokenizer saved to: {tokenizer_path}\")\n","print(\"\\nFiles in the tokenizer directory:\")\n","!ls -lh {tokenizer_path}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"inuGdOED0Cyw","executionInfo":{"status":"ok","timestamp":1749658399156,"user_tz":240,"elapsed":170,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"4a91a1ba-7569-40c3-a7c1-7909df0a615a"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Tokenizer saved to: /content/drive/MyDrive/dev/.models/TinyLlama-1.1B-Chat-v1.0_quantized_onnx/tokenizer/\n","\n","Files in the tokenizer directory:\n","total 4.0M\n","-rw------- 1 root root  551 Jun 11 16:13 special_tokens_map.json\n","-rw------- 1 root root 1.4K Jun 11 16:13 tokenizer_config.json\n","-rw------- 1 root root 3.5M Jun 11 16:13 tokenizer.json\n","-rw------- 1 root root 489K Jun 11 16:13 tokenizer.model\n"]}]},{"cell_type":"code","source":["# Load the base model\n","model = ORTModelForCausalLM.from_pretrained(source_path, export=True)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"Lnq9aCCXjFYA","executionInfo":{"status":"ok","timestamp":1749658491488,"user_tz":240,"elapsed":87693,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"a88a90df-d5f8-426f-ad99-a51c45a7a341"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.11/dist-packages/transformers/cache_utils.py:457: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  or not self.key_cache[layer_idx].numel()  # the layer has no cache\n","/usr/local/lib/python3.11/dist-packages/transformers/models/llama/modeling_llama.py:712: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  if sequence_length != 1:\n","/usr/local/lib/python3.11/dist-packages/transformers/cache_utils.py:440: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  elif (\n","/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py:47: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n","  is_causal = query.shape[2] > 1 and causal_mask is None\n"]}]},{"cell_type":"code","source":["# Save ONNX unquantized model to local drive\n","# model.save_pretrained(unquantized_path)\n","\n","# print(f\"Unquantized ONNX model saved to: {unquantized_path}\")\n","# print(\"\\nFiles in the unquantized directory:\")\n","# !ls -lh {unquantized_path}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"A9FeK_Pn3PT2","executionInfo":{"status":"ok","timestamp":1749656432238,"user_tz":240,"elapsed":15097,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"fb0def9d-600e-42f3-f4cc-c83a90e8b040"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Unquantized ONNX model saved to: /content/drive/MyDrive/dev/.models/TinyLlama-1.1B-Chat-v1.0_quantized_onnx/onnx_unquantized/\n","\n","Files in the unquantized directory:\n","total 4.1G\n","-rw------- 1 root root  675 Jun 11 15:40 config.json\n","-rw------- 1 root root  124 Jun 11 15:40 generation_config.json\n","-rw------- 1 root root 965K Jun 11 15:40 model.onnx\n","-rw------- 1 root root 4.1G Jun 11 15:40 model.onnx_data\n"]}]},{"cell_type":"code","source":["# Create a quantizer object from loading the ONNX unquantized model\n","quantizer = ORTQuantizer.from_pretrained(model)"],"metadata":{"id":"6MWHzPEr3hRu"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"yGufhtIOy9wZ"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Create the quantization config using a pre-made recipe for dynamic quantization on CPU\n","# is_static=True optimizes the quantized model upfront for faster inference\n","# is_static=False allows the quantized model to optimize on the fly, with slower inference\n","quantization_config = AutoQuantizationConfig.avx512_vnni(is_static=False)"],"metadata":{"id":"tp9oDJfF3hUS"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Apply quantization and save the new, smaller model optimized for CPU\n","quantizer.quantize(\n","    save_dir=model_cpu,\n","    quantization_config=quantization_config,\n",")\n","\n","print(f\"Quantized model saved to: {model_cpu}\")\n","print(\"\\nFiles in your final quantized directory:\")\n","!ls -lh {model_cpu}"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"rjlStfRo30j4","executionInfo":{"status":"ok","timestamp":1749657464489,"user_tz":240,"elapsed":94523,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"5aa861b9-005f-4c49-c6a8-45d8e86f1a47"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Quantized model saved to: /content/drive/MyDrive/dev/.models/TinyLlama-1.1B-Chat-v1.0_quantized_onnx/model_cpu/\n","\n","Files in your final quantized directory:\n","total 1.1G\n","-rw------- 1 root root  675 Jun 11 15:57 config.json\n","-rw------- 1 root root 1.1G Jun 11 15:57 model_quantized.onnx\n","-rw------- 1 root root  762 Jun 11 15:57 ort_config.json\n","-rw------- 1 root root  551 Jun 11 15:57 special_tokens_map.json\n","-rw------- 1 root root 1.4K Jun 11 15:57 tokenizer_config.json\n","-rw------- 1 root root 3.5M Jun 11 15:57 tokenizer.json\n","-rw------- 1 root root 489K Jun 11 15:57 tokenizer.model\n"]}]},{"cell_type":"markdown","source":["## inference"],"metadata":{"id":"I5IG-66AqAvU"}},{"cell_type":"code","source":["# Load the tokenizer directly from local drive\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"],"metadata":{"id":"_qCUvDU_gKxj"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Load the quantized model  directly from local drive\n","model = ORTModelForCausalLM.from_pretrained(model_cpu)"],"metadata":{"id":"2sObxCk249h_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["generator = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HY4NPmnY5U3H","executionInfo":{"status":"ok","timestamp":1749660415455,"user_tz":240,"elapsed":6,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"df01ef84-a8c8-47dc-ca20-beb11cb14e2b"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]}]},{"cell_type":"code","source":["prompt = \"The main benefits of using a small language model are\"\n","prompt = \"What's the meaning of life?\"\n","prompt = \"1 2 3 4 5 6 6 6 6 5 5 5 5 7 7 7 7 7 8 8 8 8 5 5 5 5 6 6 6 6 7 7 7 7 4 4 4 4 3 3 3 3 5 5 5 5 2 2 2 2 1\"\n","prompt = \"1 2 3 7 5 6 1 2 3 4 1 2\""],"metadata":{"id":"tGcyi7388BDa"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = generator(prompt, max_new_tokens=100)\n","\n","print(\"\\n--- INFERENCE RESULT ---\")\n","print(result[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"qqjN5pGp5U6I","executionInfo":{"status":"ok","timestamp":1749660543484,"user_tz":240,"elapsed":4953,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"7c41aa89-6895-45ee-ee6d-cedf8251820d"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","--- INFERENCE RESULT ---\n","1 2 3 7 5 6 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4\n"]}]},{"cell_type":"markdown","source":["# OpenVINO/TinyLlama-1.1B-Chat-v1.0-int8-ov"],"metadata":{"id":"7gAJYADn7cr9"}},{"cell_type":"markdown","source":["https://huggingface.co/OpenVINO/TinyLlama-1.1B-Chat-v1.0-int8-ov"],"metadata":{"id":"7CppWY5SCGNa"}},{"cell_type":"code","source":["!pip install -q \"optimum[openvino]\" \"transformers\" \"openvino-tokenizers\""],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8-lX-8dq7cML","executionInfo":{"status":"ok","timestamp":1749661231971,"user_tz":240,"elapsed":82298,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"7bf5e51d-b94e-4f42-885b-40ca79fd6132"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/41.9 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m107.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 MB\u001b[0m \u001b[31m57.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.6/342.6 kB\u001b[0m \u001b[31m22.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m132.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m429.3/429.3 kB\u001b[0m \u001b[31m31.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m3.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m100.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m108.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m47.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m43.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m77.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m56.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m85.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m29.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m80.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m23.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, pipeline\n","from optimum.intel import OVModelForCausalLM"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jeJMTYSr7cPh","executionInfo":{"status":"ok","timestamp":1749661363936,"user_tz":240,"elapsed":18878,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"b97c52d7-3114-42df-ca8d-fbb0df302df8"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"]}]},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/dev/.models/TinyLlama-1.1B-Chat-v1.0-int8-ov\""],"metadata":{"id":"gTiGfrMy7wKm"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# For OpenVINO models, it's recommended to use the model's tokenizer if available\n","tokenizer = AutoTokenizer.from_pretrained(model_path)"],"metadata":{"id":"iAPiIy4b7wNs"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# The key step: Use OVModelForCausalLM to load the optimized model\n","model = OVModelForCausalLM.from_pretrained(model_path)"],"metadata":{"id":"q8AG7e-M7wQC"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9OExGpmr8ADz","executionInfo":{"status":"ok","timestamp":1749661398029,"user_tz":240,"elapsed":4,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"d20d46e8-3e92-486a-b426-42a2f6387f79"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]}]},{"cell_type":"code","source":["prompt = \"The best advice for a new programmer is\"\n","prompt = \"1 2 3 7 5 6 1 2 3 4 1 2\""],"metadata":{"id":"kVtsy1kW8AGl"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["result = pipe(prompt, max_new_tokens=100)\n","\n","print(\"--- Model Output ---\")\n","print(result[0]['generated_text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"lu0sqQtN8AJn","executionInfo":{"status":"ok","timestamp":1749661430880,"user_tz":240,"elapsed":5015,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"704f443f-5ea3-498c-8b5d-1678142dd935"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["--- Model Output ---\n","1 2 3 7 5 6 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4 1 2 3 4\n"]}]},{"cell_type":"markdown","source":["# TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"],"metadata":{"id":"evocIFoO9BlE"}},{"cell_type":"markdown","source":["https://huggingface.co/TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"],"metadata":{"id":"HoTCj0WgBrD-"}},{"cell_type":"code","source":["# 1. Install the necessary library# 1. Install the necessary library\n","# llama-cpp-python is specifically designed to run GGUF models efficiently.\n","# The CMAKE_ARGS are flags to compile the library to take advantage of modern CPU features.\n","!CMAKE_ARGS=\"-DLLAMA_CUBLAS=OFF -DLLAMA_CUDA_F16=OFF -DLLAMA_AVX=ON -DLLAMA_AVX2=ON -DLLAMA_F16C=ON -DLLAMA_FMA=ON -DLLAMA_SSE3=ON -DLLAMA_SSSE3=ON\" pip install llama-cpp-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HmTvs9Hs8RxY","executionInfo":{"status":"ok","timestamp":1749662331215,"user_tz":240,"elapsed":78533,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"ce6378b0-267c-4511-df6d-306991d5b84f"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m33.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.9-cp311-cp311-linux_x86_64.whl size=4067751 sha256=186401f69ff5a5e603b07d506b74e91963b46d79c2dbd0482391eb05bd8b7788\n","  Stored in directory: /root/.cache/pip/wheels/9e/8f/bf/148c8eb7d69021eccd6eae6444f3accd48347587054ffd24e5\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.9\n"]}]},{"cell_type":"code","source":["from llama_cpp import Llama"],"metadata":{"id":"r3KJYSFTAROU"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["#model_path = \"/content/drive/MyDrive/dev/.models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n","model_path = \"/content/drive/MyDrive/dev/.models/tinyllama-1.1b-chat-v1.0.Q2_K.gguf\""],"metadata":{"id":"EDZKqaIy8SBD"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# 6. Load the GGUF model\n","#    n_ctx: The context window size (max number of tokens).\n","#    n_threads: Number of CPU threads to use. Colab usually has 2.\n","print(\"Loading GGUF model...\")\n","llm = Llama(\n","    model_path=model_path,\n","    n_ctx=2048,\n","    n_threads=2,\n",")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"DaOIIQ1l_6wW","executionInfo":{"status":"ok","timestamp":1749662791266,"user_tz":240,"elapsed":6267,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"4a11633a-d02b-4975-eef9-990b6605f489"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Loading GGUF model...\n"]},{"output_type":"stream","name":"stderr","text":["llama_model_loader: loaded meta data with 23 key-value pairs and 201 tensors from /content/drive/MyDrive/dev/.models/tinyllama-1.1b-chat-v1.0.Q2_K.gguf (version GGUF V3 (latest))\n","llama_model_loader: Dumping metadata keys/values. Note: KV overrides do not apply in this output.\n","llama_model_loader: - kv   0:                       general.architecture str              = llama\n","llama_model_loader: - kv   1:                               general.name str              = tinyllama_tinyllama-1.1b-chat-v1.0\n","llama_model_loader: - kv   2:                       llama.context_length u32              = 2048\n","llama_model_loader: - kv   3:                     llama.embedding_length u32              = 2048\n","llama_model_loader: - kv   4:                          llama.block_count u32              = 22\n","llama_model_loader: - kv   5:                  llama.feed_forward_length u32              = 5632\n","llama_model_loader: - kv   6:                 llama.rope.dimension_count u32              = 64\n","llama_model_loader: - kv   7:                 llama.attention.head_count u32              = 32\n","llama_model_loader: - kv   8:              llama.attention.head_count_kv u32              = 4\n","llama_model_loader: - kv   9:     llama.attention.layer_norm_rms_epsilon f32              = 0.000010\n","llama_model_loader: - kv  10:                       llama.rope.freq_base f32              = 10000.000000\n","llama_model_loader: - kv  11:                          general.file_type u32              = 10\n","llama_model_loader: - kv  12:                       tokenizer.ggml.model str              = llama\n","llama_model_loader: - kv  13:                      tokenizer.ggml.tokens arr[str,32000]   = [\"<unk>\", \"<s>\", \"</s>\", \"<0x00>\", \"<...\n","llama_model_loader: - kv  14:                      tokenizer.ggml.scores arr[f32,32000]   = [0.000000, 0.000000, 0.000000, 0.0000...\n","llama_model_loader: - kv  15:                  tokenizer.ggml.token_type arr[i32,32000]   = [2, 3, 3, 6, 6, 6, 6, 6, 6, 6, 6, 6, ...\n","llama_model_loader: - kv  16:                      tokenizer.ggml.merges arr[str,61249]   = [\"▁ t\", \"e r\", \"i n\", \"▁ a\", \"e n...\n","llama_model_loader: - kv  17:                tokenizer.ggml.bos_token_id u32              = 1\n","llama_model_loader: - kv  18:                tokenizer.ggml.eos_token_id u32              = 2\n","llama_model_loader: - kv  19:            tokenizer.ggml.unknown_token_id u32              = 0\n","llama_model_loader: - kv  20:            tokenizer.ggml.padding_token_id u32              = 2\n","llama_model_loader: - kv  21:                    tokenizer.chat_template str              = {% for message in messages %}\\n{% if m...\n","llama_model_loader: - kv  22:               general.quantization_version u32              = 2\n","llama_model_loader: - type  f32:   45 tensors\n","llama_model_loader: - type q2_K:   45 tensors\n","llama_model_loader: - type q3_K:  110 tensors\n","llama_model_loader: - type q6_K:    1 tensors\n","print_info: file format = GGUF V3 (latest)\n","print_info: file type   = Q2_K - Medium\n","print_info: file size   = 459.11 MiB (3.50 BPW) \n","init_tokenizer: initializing tokenizer for type 1\n","load: control token:      2 '</s>' is not marked as EOG\n","load: control token:      1 '<s>' is not marked as EOG\n","load: special_eos_id is not in special_eog_ids - the tokenizer config may be incorrect\n","load: special tokens cache size = 3\n","load: token to piece cache size = 0.1684 MB\n","print_info: arch             = llama\n","print_info: vocab_only       = 0\n","print_info: n_ctx_train      = 2048\n","print_info: n_embd           = 2048\n","print_info: n_layer          = 22\n","print_info: n_head           = 32\n","print_info: n_head_kv        = 4\n","print_info: n_rot            = 64\n","print_info: n_swa            = 0\n","print_info: n_swa_pattern    = 1\n","print_info: n_embd_head_k    = 64\n","print_info: n_embd_head_v    = 64\n","print_info: n_gqa            = 8\n","print_info: n_embd_k_gqa     = 256\n","print_info: n_embd_v_gqa     = 256\n","print_info: f_norm_eps       = 0.0e+00\n","print_info: f_norm_rms_eps   = 1.0e-05\n","print_info: f_clamp_kqv      = 0.0e+00\n","print_info: f_max_alibi_bias = 0.0e+00\n","print_info: f_logit_scale    = 0.0e+00\n","print_info: f_attn_scale     = 0.0e+00\n","print_info: n_ff             = 5632\n","print_info: n_expert         = 0\n","print_info: n_expert_used    = 0\n","print_info: causal attn      = 1\n","print_info: pooling type     = 0\n","print_info: rope type        = 0\n","print_info: rope scaling     = linear\n","print_info: freq_base_train  = 10000.0\n","print_info: freq_scale_train = 1\n","print_info: n_ctx_orig_yarn  = 2048\n","print_info: rope_finetuned   = unknown\n","print_info: ssm_d_conv       = 0\n","print_info: ssm_d_inner      = 0\n","print_info: ssm_d_state      = 0\n","print_info: ssm_dt_rank      = 0\n","print_info: ssm_dt_b_c_rms   = 0\n","print_info: model type       = 1B\n","print_info: model params     = 1.10 B\n","print_info: general.name     = tinyllama_tinyllama-1.1b-chat-v1.0\n","print_info: vocab type       = SPM\n","print_info: n_vocab          = 32000\n","print_info: n_merges         = 0\n","print_info: BOS token        = 1 '<s>'\n","print_info: EOS token        = 2 '</s>'\n","print_info: UNK token        = 0 '<unk>'\n","print_info: PAD token        = 2 '</s>'\n","print_info: LF token         = 13 '<0x0A>'\n","print_info: EOG token        = 2 '</s>'\n","print_info: max token length = 48\n","load_tensors: loading model tensors, this can take a while... (mmap = true)\n","load_tensors: layer   0 assigned to device CPU, is_swa = 0\n","load_tensors: layer   1 assigned to device CPU, is_swa = 0\n","load_tensors: layer   2 assigned to device CPU, is_swa = 0\n","load_tensors: layer   3 assigned to device CPU, is_swa = 0\n","load_tensors: layer   4 assigned to device CPU, is_swa = 0\n","load_tensors: layer   5 assigned to device CPU, is_swa = 0\n","load_tensors: layer   6 assigned to device CPU, is_swa = 0\n","load_tensors: layer   7 assigned to device CPU, is_swa = 0\n","load_tensors: layer   8 assigned to device CPU, is_swa = 0\n","load_tensors: layer   9 assigned to device CPU, is_swa = 0\n","load_tensors: layer  10 assigned to device CPU, is_swa = 0\n","load_tensors: layer  11 assigned to device CPU, is_swa = 0\n","load_tensors: layer  12 assigned to device CPU, is_swa = 0\n","load_tensors: layer  13 assigned to device CPU, is_swa = 0\n","load_tensors: layer  14 assigned to device CPU, is_swa = 0\n","load_tensors: layer  15 assigned to device CPU, is_swa = 0\n","load_tensors: layer  16 assigned to device CPU, is_swa = 0\n","load_tensors: layer  17 assigned to device CPU, is_swa = 0\n","load_tensors: layer  18 assigned to device CPU, is_swa = 0\n","load_tensors: layer  19 assigned to device CPU, is_swa = 0\n","load_tensors: layer  20 assigned to device CPU, is_swa = 0\n","load_tensors: layer  21 assigned to device CPU, is_swa = 0\n","load_tensors: layer  22 assigned to device CPU, is_swa = 0\n","load_tensors: tensor 'token_embd.weight' (q2_K) (and 200 others) cannot be used with preferred buffer type CPU_AARCH64, using CPU instead\n","load_tensors:   CPU_Mapped model buffer size =   459.11 MiB\n","......................................................................................\n","llama_context: constructing llama_context\n","llama_context: n_seq_max     = 1\n","llama_context: n_ctx         = 2048\n","llama_context: n_ctx_per_seq = 2048\n","llama_context: n_batch       = 512\n","llama_context: n_ubatch      = 512\n","llama_context: causal_attn   = 1\n","llama_context: flash_attn    = 0\n","llama_context: freq_base     = 10000.0\n","llama_context: freq_scale    = 1\n","set_abort_callback: call\n","llama_context:        CPU  output buffer size =     0.12 MiB\n","create_memory: n_ctx = 2048 (padded)\n","llama_kv_cache_unified: kv_size = 2048, type_k = 'f16', type_v = 'f16', n_layer = 22, can_shift = 1, padding = 32\n","llama_kv_cache_unified: layer   0: dev = CPU\n","llama_kv_cache_unified: layer   1: dev = CPU\n","llama_kv_cache_unified: layer   2: dev = CPU\n","llama_kv_cache_unified: layer   3: dev = CPU\n","llama_kv_cache_unified: layer   4: dev = CPU\n","llama_kv_cache_unified: layer   5: dev = CPU\n","llama_kv_cache_unified: layer   6: dev = CPU\n","llama_kv_cache_unified: layer   7: dev = CPU\n","llama_kv_cache_unified: layer   8: dev = CPU\n","llama_kv_cache_unified: layer   9: dev = CPU\n","llama_kv_cache_unified: layer  10: dev = CPU\n","llama_kv_cache_unified: layer  11: dev = CPU\n","llama_kv_cache_unified: layer  12: dev = CPU\n","llama_kv_cache_unified: layer  13: dev = CPU\n","llama_kv_cache_unified: layer  14: dev = CPU\n","llama_kv_cache_unified: layer  15: dev = CPU\n","llama_kv_cache_unified: layer  16: dev = CPU\n","llama_kv_cache_unified: layer  17: dev = CPU\n","llama_kv_cache_unified: layer  18: dev = CPU\n","llama_kv_cache_unified: layer  19: dev = CPU\n","llama_kv_cache_unified: layer  20: dev = CPU\n","llama_kv_cache_unified: layer  21: dev = CPU\n","llama_kv_cache_unified:        CPU KV buffer size =    44.00 MiB\n","llama_kv_cache_unified: KV self size  =   44.00 MiB, K (f16):   22.00 MiB, V (f16):   22.00 MiB\n","llama_context: enumerating backends\n","llama_context: backend_ptrs.size() = 1\n","llama_context: max_nodes = 65536\n","llama_context: worst-case: n_tokens = 512, n_seqs = 1, n_outputs = 0\n","llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n","llama_context: reserving graph for n_tokens = 1, n_seqs = 1\n","llama_context: reserving graph for n_tokens = 512, n_seqs = 1\n","llama_context:        CPU compute buffer size =   148.01 MiB\n","llama_context: graph nodes  = 754\n","llama_context: graph splits = 1\n","CPU : SSE3 = 1 | SSSE3 = 1 | AVX = 1 | AVX2 = 1 | F16C = 1 | FMA = 1 | BMI2 = 1 | LLAMAFILE = 1 | OPENMP = 1 | AARCH64_REPACK = 1 | \n","Model metadata: {'tokenizer.chat_template': \"{% for message in messages %}\\n{% if message['role'] == 'user' %}\\n{{ '<|user|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'system' %}\\n{{ '<|system|>\\n' + message['content'] + eos_token }}\\n{% elif message['role'] == 'assistant' %}\\n{{ '<|assistant|>\\n'  + message['content'] + eos_token }}\\n{% endif %}\\n{% if loop.last and add_generation_prompt %}\\n{{ '<|assistant|>' }}\\n{% endif %}\\n{% endfor %}\", 'tokenizer.ggml.padding_token_id': '2', 'tokenizer.ggml.unknown_token_id': '0', 'tokenizer.ggml.eos_token_id': '2', 'general.architecture': 'llama', 'llama.rope.freq_base': '10000.000000', 'llama.context_length': '2048', 'general.name': 'tinyllama_tinyllama-1.1b-chat-v1.0', 'llama.embedding_length': '2048', 'llama.feed_forward_length': '5632', 'llama.attention.layer_norm_rms_epsilon': '0.000010', 'llama.rope.dimension_count': '64', 'tokenizer.ggml.bos_token_id': '1', 'llama.attention.head_count': '32', 'llama.block_count': '22', 'llama.attention.head_count_kv': '4', 'general.quantization_version': '2', 'tokenizer.ggml.model': 'llama', 'general.file_type': '10'}\n","Available chat formats from metadata: chat_template.default\n","Using gguf chat template: {% for message in messages %}\n","{% if message['role'] == 'user' %}\n","{{ '<|user|>\n","' + message['content'] + eos_token }}\n","{% elif message['role'] == 'system' %}\n","{{ '<|system|>\n","' + message['content'] + eos_token }}\n","{% elif message['role'] == 'assistant' %}\n","{{ '<|assistant|>\n","'  + message['content'] + eos_token }}\n","{% endif %}\n","{% if loop.last and add_generation_prompt %}\n","{{ '<|assistant|>' }}\n","{% endif %}\n","{% endfor %}\n","Using chat eos_token: </s>\n","Using chat bos_token: <s>\n"]}]},{"cell_type":"code","source":["# 7. Set up for inference and run a prompt\n","prompt = \"The best advice for a new programmer is\"\n","# Note: Llama.cpp uses a specific chat format. We can create it manually.\n","# For TinyLlama, the format is: <|system|>\\n{system_prompt}<|end|>\\n<|user|>\\n{user_prompt}<|end|>\\n<|assistant|>\n","prompt_template = f\"<|system|>\\nYou are a helpful assistant.<|end|>\\n<|user|>\\n{prompt}<|end|>\\n<|assistant|>\""],"metadata":{"id":"2sP9hL1h_64_"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["print(f\"\\nGenerating response for prompt: '{prompt}'\")\n","response = llm(\n","    prompt_template,\n","    max_tokens=100,\n","    stop=[\"<|end|>\", \"user:\"], # Stop generating when the model thinks its turn is over\n","    echo=False # Don't repeat the prompt in the output\n",")\n","\n","print(\"\\n--- Model Output ---\")\n","print(response[\"choices\"][0][\"text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"e98W3pHm8SEG","executionInfo":{"status":"ok","timestamp":1749662798886,"user_tz":240,"elapsed":2870,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"88345746-f28c-4512-bf38-61b55288dd01"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["\n","Generating response for prompt: 'The best advice for a new programmer is'\n"]},{"output_type":"stream","name":"stderr","text":["llama_perf_context_print:        load time =     754.07 ms\n","llama_perf_context_print: prompt eval time =     753.85 ms /    44 tokens (   17.13 ms per token,    58.37 tokens per second)\n","llama_perf_context_print:        eval time =    2092.19 ms /    41 runs   (   51.03 ms per token,    19.60 tokens per second)\n","llama_perf_context_print:       total time =    2867.08 ms /    85 tokens\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Model Output ---\n","\n","It's a cliché, but it's a great piece of advice. It's a reminder that a new programmer can learn and grow, and that their efforts are appreciated.\n"]}]},{"cell_type":"code","source":["prompt = \"1 2 3 4 5 6\""],"metadata":{"id":"nrAWGripA2cp"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["response = llm(\n","    prompt,\n","    max_tokens=100,\n","    stop=[\"\\n\"], # Stop generating when the model thinks its turn is over\n","    echo=False # Don't repeat the prompt in the output\n",")\n","\n","print(\"\\n--- Model Output ---\")\n","print(response[\"choices\"][0][\"text\"])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"9ObJ-M0VA2fO","executionInfo":{"status":"ok","timestamp":1749662823852,"user_tz":240,"elapsed":4234,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"e11cf59a-8cd9-4f41-fbb9-ec66a3473f87"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stderr","text":["Llama.generate: 1 prefix-match hit, remaining 12 prompt tokens to eval\n","llama_perf_context_print:        load time =     754.07 ms\n","llama_perf_context_print: prompt eval time =     682.88 ms /    12 tokens (   56.91 ms per token,    17.57 tokens per second)\n","llama_perf_context_print:        eval time =    3468.65 ms /    69 runs   (   50.27 ms per token,    19.89 tokens per second)\n","llama_perf_context_print:       total time =    4188.05 ms /    81 tokens\n"]},{"output_type":"stream","name":"stdout","text":["\n","--- Model Output ---\n"," 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30\n"]}]},{"cell_type":"markdown","source":["# Tests"],"metadata":{"id":"-aWqYvAi7XXK"}},{"cell_type":"markdown","source":["## Setup"],"metadata":{"id":"RoGwr0WcWxR5"}},{"cell_type":"code","source":["!pip install -U \"optimum[onnxruntime]\" \"optimum[openvino]\" \"transformers\" \"openvino-tokenizers\""],"metadata":{"id":"7qSntbXTBNv2","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749730237978,"user_tz":240,"elapsed":135143,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"077d4766-a27b-4707-c433-78f4bf0bc8d0"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n","Collecting openvino-tokenizers\n","  Downloading openvino_tokenizers-2025.1.0.0-py3-none-manylinux2014_x86_64.whl.metadata (41 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.9/41.9 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting optimum[onnxruntime]\n","  Downloading optimum-1.25.3-py3-none-any.whl.metadata (16 kB)\n","Requirement already satisfied: torch>=1.11 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (2.6.0+cu124)\n","Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (24.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (2.0.2)\n","Requirement already satisfied: huggingface-hub>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (0.32.4)\n","Collecting onnx (from optimum[onnxruntime])\n","  Downloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.9 kB)\n","Requirement already satisfied: datasets>=1.2.1 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (2.14.4)\n","Requirement already satisfied: protobuf>=3.20.1 in /usr/local/lib/python3.11/dist-packages (from optimum[onnxruntime]) (5.29.5)\n","Collecting onnxruntime>=1.11.0 (from optimum[onnxruntime])\n","  Downloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n","Collecting transformers\n","  Downloading transformers-4.51.3-py3-none-any.whl.metadata (38 kB)\n","Collecting optimum-intel>=1.23.0 (from optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino])\n","  Downloading optimum_intel-1.23.0-py3-none-any.whl.metadata (14 kB)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n","Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n","Collecting openvino~=2025.1.0.dev (from openvino-tokenizers)\n","  Downloading openvino-2025.1.0-18503-cp311-cp311-manylinux2014_x86_64.whl.metadata (8.5 kB)\n","Requirement already satisfied: pyarrow>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (18.1.0)\n","Requirement already satisfied: dill<0.3.8,>=0.3.0 in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.3.7)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (2.2.2)\n","Requirement already satisfied: xxhash in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (3.5.0)\n","Requirement already satisfied: multiprocess in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (0.70.15)\n","Requirement already satisfied: fsspec>=2021.11.1 in /usr/local/lib/python3.11/dist-packages (from fsspec[http]>=2021.11.1->datasets>=1.2.1->optimum[onnxruntime]) (2025.3.2)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets>=1.2.1->optimum[onnxruntime]) (3.11.15)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime]) (4.14.0)\n","Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.8.0->optimum[onnxruntime]) (1.1.2)\n","Collecting coloredlogs (from onnxruntime>=1.11.0->optimum[onnxruntime])\n","  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n","Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (25.2.10)\n","Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime>=1.11.0->optimum[onnxruntime]) (1.13.1)\n","Collecting openvino-telemetry>=2023.2.1 (from openvino~=2025.1.0.dev->openvino-tokenizers)\n","  Downloading openvino_telemetry-2025.1.0-py3-none-any.whl.metadata (2.3 kB)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from optimum-intel>=1.23.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (75.2.0)\n","Requirement already satisfied: scipy in /usr/local/lib/python3.11/dist-packages (from optimum-intel>=1.23.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (1.15.3)\n","Collecting nncf>=2.16.0 (from optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino])\n","  Downloading nncf-2.16.0-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.4.26)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.5)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.1.6)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch>=1.11->optimum[onnxruntime])\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch>=1.11->optimum[onnxruntime])\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch>=1.11->optimum[onnxruntime])\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch>=1.11->optimum[onnxruntime])\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch>=1.11->optimum[onnxruntime])\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch>=1.11->optimum[onnxruntime])\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch>=1.11->optimum[onnxruntime])\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch>=1.11->optimum[onnxruntime])\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch>=1.11->optimum[onnxruntime])\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch>=1.11->optimum[onnxruntime])\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.11->optimum[onnxruntime]) (3.2.0)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime>=1.11.0->optimum[onnxruntime]) (1.3.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (25.3.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (1.6.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (6.4.4)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (0.3.1)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets>=1.2.1->optimum[onnxruntime]) (1.20.0)\n","Requirement already satisfied: jsonschema>=3.2.0 in /usr/local/lib/python3.11/dist-packages (from nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (4.24.0)\n","Collecting jstyleson>=0.0.2 (from nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino])\n","  Downloading jstyleson-0.0.2.tar.gz (2.0 kB)\n","  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: natsort>=7.1.0 in /usr/local/lib/python3.11/dist-packages (from nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (8.4.0)\n","Collecting networkx (from torch>=1.11->optimum[onnxruntime])\n","  Downloading networkx-3.4.2-py3-none-any.whl.metadata (6.3 kB)\n","Collecting ninja<1.12,>=1.10.0.post2 (from nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino])\n","  Downloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (5.9.5)\n","Requirement already satisfied: pydot<=3.0.4,>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (3.0.4)\n","Collecting pymoo>=0.6.0.1 (from nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino])\n","  Downloading pymoo-0.6.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.0 kB)\n","Requirement already satisfied: rich>=13.5.2 in /usr/local/lib/python3.11/dist-packages (from nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (13.9.4)\n","Requirement already satisfied: scikit-learn>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (1.6.1)\n","Requirement already satisfied: tabulate>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (0.9.0)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2.9.0.post0)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2025.2)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets>=1.2.1->optimum[onnxruntime]) (2025.2)\n","Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime>=1.11.0->optimum[onnxruntime])\n","  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.11->optimum[onnxruntime]) (3.0.2)\n","Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.2.0->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (2025.4.1)\n","Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.2.0->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (0.36.2)\n","Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from jsonschema>=3.2.0->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (0.25.1)\n","Requirement already satisfied: pyparsing>=3.0.9 in /usr/local/lib/python3.11/dist-packages (from pydot<=3.0.4,>=1.4.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (3.2.3)\n","Requirement already satisfied: matplotlib>=3 in /usr/local/lib/python3.11/dist-packages (from pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (3.10.0)\n","Requirement already satisfied: autograd>=1.4 in /usr/local/lib/python3.11/dist-packages (from pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (1.8.0)\n","Collecting cma>=3.2.2 (from pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino])\n","  Downloading cma-4.2.0-py3-none-any.whl.metadata (7.7 kB)\n","Collecting alive-progress (from pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino])\n","  Downloading alive_progress-3.2.0-py3-none-any.whl.metadata (70 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m70.6/70.6 kB\u001b[0m \u001b[31m5.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting Deprecated (from pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino])\n","  Downloading Deprecated-1.2.18-py2.py3-none-any.whl.metadata (5.7 kB)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas->datasets>=1.2.1->optimum[onnxruntime]) (1.17.0)\n","Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.5.2->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (3.0.0)\n","Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=13.5.2->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (2.19.1)\n","Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (1.5.1)\n","Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=0.24.0->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (3.6.0)\n","Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=13.5.2->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (0.1.2)\n","Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (0.12.1)\n","Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (4.58.1)\n","Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (1.4.8)\n","Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3->pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (11.2.1)\n","Collecting about-time==4.2.1 (from alive-progress->pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino])\n","  Downloading about_time-4.2.1-py3-none-any.whl.metadata (13 kB)\n","Collecting grapheme==0.6.0 (from alive-progress->pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino])\n","  Downloading grapheme-0.6.0.tar.gz (207 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m10.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: wrapt<2,>=1.10 in /usr/local/lib/python3.11/dist-packages (from Deprecated->pymoo>=0.6.0.1->nncf>=2.16.0->optimum-intel[openvino]>=1.23.0; extra == \"openvino\"->optimum[openvino]) (1.17.2)\n","Downloading transformers-4.51.3-py3-none-any.whl (10.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.4/10.4 MB\u001b[0m \u001b[31m95.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openvino_tokenizers-2025.1.0.0-py3-none-manylinux2014_x86_64.whl (13.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.7/13.7 MB\u001b[0m \u001b[31m88.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnxruntime-1.22.0-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.4/16.4 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openvino-2025.1.0-18503-cp311-cp311-manylinux2014_x86_64.whl (46.0 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 MB\u001b[0m \u001b[31m44.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading optimum_intel-1.23.0-py3-none-any.whl (342 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m342.6/342.6 kB\u001b[0m \u001b[31m20.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading optimum-1.25.3-py3-none-any.whl (429 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m429.3/429.3 kB\u001b[0m \u001b[31m14.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m103.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m17.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading onnx-1.18.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.6/17.6 MB\u001b[0m \u001b[31m94.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nncf-2.16.0-py3-none-any.whl (1.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.4/1.4 MB\u001b[0m \u001b[31m58.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading networkx-3.4.2-py3-none-any.whl (1.7 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m60.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading openvino_telemetry-2025.1.0-py3-none-any.whl (25 kB)\n","Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m6.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading ninja-1.11.1.4-py3-none-manylinux_2_12_x86_64.manylinux2010_x86_64.whl (422 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m422.8/422.8 kB\u001b[0m \u001b[31m27.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading pymoo-0.6.1.5-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.4/4.4 MB\u001b[0m \u001b[31m87.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading cma-4.2.0-py3-none-any.whl (288 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m288.2/288.2 kB\u001b[0m \u001b[31m19.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading alive_progress-3.2.0-py3-none-any.whl (77 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.1/77.1 kB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading about_time-4.2.1-py3-none-any.whl (13 kB)\n","Downloading Deprecated-1.2.18-py2.py3-none-any.whl (10.0 kB)\n","Building wheels for collected packages: jstyleson, grapheme\n","  Building wheel for jstyleson (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for jstyleson: filename=jstyleson-0.0.2-py3-none-any.whl size=2383 sha256=0350133b7d08962542347089d273ca1dac6b74822b0ad2580c7cabe0c6be8082\n","  Stored in directory: /root/.cache/pip/wheels/ad/63/0e/50090147fb424100f7d9078b71c21b9e7468b6f643515a60d6\n","  Building wheel for grapheme (setup.py) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for grapheme: filename=grapheme-0.6.0-py3-none-any.whl size=210082 sha256=4d2806fb50a5b2d4efef97bf2e33284c306cf563ecf38f0ae4f105181d273e3a\n","  Stored in directory: /root/.cache/pip/wheels/ee/3b/0b/1b865800e916d671a24028d884698674138632a83fdfad4926\n","Successfully built jstyleson grapheme\n","Installing collected packages: openvino-telemetry, jstyleson, grapheme, openvino, onnx, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, ninja, networkx, humanfriendly, Deprecated, cma, about-time, openvino-tokenizers, nvidia-cusparse-cu12, nvidia-cudnn-cu12, coloredlogs, alive-progress, pymoo, onnxruntime, nvidia-cusolver-cu12, transformers, nncf, optimum, optimum-intel\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: networkx\n","    Found existing installation: networkx 3.5\n","    Uninstalling networkx-3.5:\n","      Successfully uninstalled networkx-3.5\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","  Attempting uninstall: transformers\n","    Found existing installation: transformers 4.52.4\n","    Uninstalling transformers-4.52.4:\n","      Successfully uninstalled transformers-4.52.4\n","Successfully installed Deprecated-1.2.18 about-time-4.2.1 alive-progress-3.2.0 cma-4.2.0 coloredlogs-15.0.1 grapheme-0.6.0 humanfriendly-10.0 jstyleson-0.0.2 networkx-3.4.2 ninja-1.11.1.4 nncf-2.16.0 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 onnx-1.18.0 onnxruntime-1.22.0 openvino-2025.1.0 openvino-telemetry-2025.1.0 openvino-tokenizers-2025.1.0.0 optimum-1.25.3 optimum-intel-1.23.0 pymoo-0.6.1.5 transformers-4.51.3\n"]}]},{"cell_type":"code","source":["!CMAKE_ARGS=\"-DLLAMA_CUBLAS=OFF -DLLAMA_CUDA_F16=OFF -DLLAMA_AVX=ON -DLLAMA_AVX2=ON -DLLAMA_F16C=ON -DLLAMA_FMA=ON -DLLAMA_SSE3=ON -DLLAMA_SSSE3=ON\" pip install llama-cpp-python"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IYambgx590u2","executionInfo":{"status":"ok","timestamp":1749730341163,"user_tz":240,"elapsed":103187,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"4863e61e-18f5-4d6e-9c7d-422ac7b5138d"},"execution_count":3,"outputs":[{"output_type":"stream","name":"stdout","text":["Collecting llama-cpp-python\n","  Downloading llama_cpp_python-0.3.9.tar.gz (67.9 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m67.9/67.9 MB\u001b[0m \u001b[31m16.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n","  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n","  Installing backend dependencies ... \u001b[?25l\u001b[?25hdone\n","  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","Requirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hBuilding wheels for collected packages: llama-cpp-python\n","  Building wheel for llama-cpp-python (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","  Created wheel for llama-cpp-python: filename=llama_cpp_python-0.3.9-cp311-cp311-linux_x86_64.whl size=4067704 sha256=569aeeb8acb4bdf51539b03d1dba5a0de864d09040b9a12193568879215d918a\n","  Stored in directory: /root/.cache/pip/wheels/9e/8f/bf/148c8eb7d69021eccd6eae6444f3accd48347587054ffd24e5\n","Successfully built llama-cpp-python\n","Installing collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.9\n"]}]},{"cell_type":"code","source":["import torch\n","from transformers import AutoTokenizer, pipeline"],"metadata":{"id":"ZfmG9YnN8Xe4","executionInfo":{"status":"ok","timestamp":1749730364096,"user_tz":240,"elapsed":22898,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":["## Load Model: TinyLlama/TinyLlama-1.1B-Chat-v1.0 (quantized)"],"metadata":{"id":"1PH8miFH7rzt"}},{"cell_type":"code","source":["from optimum.onnxruntime import ORTModelForCausalLM, ORTQuantizer\n","from optimum.onnxruntime import AutoQuantizationConfig"],"metadata":{"id":"Plpq-LTl7hZ6","executionInfo":{"status":"ok","timestamp":1749730371463,"user_tz":240,"elapsed":7374,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":5,"outputs":[]},{"cell_type":"code","source":["# Create save_path only if it does not already exist\n","save_path = '/content/drive/MyDrive/dev/.models/TinyLlama-1.1B-Chat-v1.0_quantized_onnx/'\n","\n","# Add tokenizer model directory inside save_path\n","tokenizer_path = save_path+'tokenizer/'\n","\n","# Add CPU optimized quantized model directory inside save_path\n","model_cpu = save_path+'model_cpu/'"],"metadata":{"id":"XNxspefu8rrn","executionInfo":{"status":"ok","timestamp":1749730373318,"user_tz":240,"elapsed":1854,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":6,"outputs":[]},{"cell_type":"code","source":["# Load the tokenizer directly from local drive\n","tokenizer = AutoTokenizer.from_pretrained(tokenizer_path)"],"metadata":{"id":"qX39o8Gw7qyN","executionInfo":{"status":"ok","timestamp":1749730410644,"user_tz":240,"elapsed":204,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":10,"outputs":[]},{"cell_type":"code","source":["# Load the quantized model  directly from local drive\n","model = ORTModelForCausalLM.from_pretrained(model_cpu)"],"metadata":{"id":"tdlGR0Vt7qyO","executionInfo":{"status":"ok","timestamp":1749730394655,"user_tz":240,"elapsed":20170,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":8,"outputs":[]},{"cell_type":"code","source":["generator_quantized_onnx = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749730412397,"user_tz":240,"elapsed":8,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"27b08338-3184-4a21-976d-36743f59a226","id":"dgOINM477qyO"},"execution_count":11,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]}]},{"cell_type":"markdown","source":["## Load Model: OpenVINO/TinyLlama-1.1B-Chat-v1.0-int8-ov"],"metadata":{"id":"fvHgLMX-7xb8"}},{"cell_type":"code","source":["from optimum.intel import OVModelForCausalLM"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749730420440,"user_tz":240,"elapsed":3638,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"2f119559-c49e-4ee1-f53f-76ccdc6a2f4d","id":"cKDBZ0Fs8AFk"},"execution_count":12,"outputs":[{"output_type":"stream","name":"stderr","text":["No CUDA runtime is found, using CUDA_HOME='/usr/local/cuda'\n"]}]},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/dev/.models/TinyLlama-1.1B-Chat-v1.0-int8-ov\""],"metadata":{"id":"6R-ciVvF8AFk","executionInfo":{"status":"ok","timestamp":1749730420454,"user_tz":240,"elapsed":2,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":13,"outputs":[]},{"cell_type":"code","source":["# For OpenVINO models, it's recommended to use the model's tokenizer if available\n","tokenizer = AutoTokenizer.from_pretrained(model_path)"],"metadata":{"id":"lPi-yumt8AFk","executionInfo":{"status":"ok","timestamp":1749730421094,"user_tz":240,"elapsed":639,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":14,"outputs":[]},{"cell_type":"code","source":["# The key step: Use OVModelForCausalLM to load the optimized model\n","model = OVModelForCausalLM.from_pretrained(model_path)"],"metadata":{"id":"OweBinNU8AFk","executionInfo":{"status":"ok","timestamp":1749730446308,"user_tz":240,"elapsed":25216,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":15,"outputs":[]},{"cell_type":"code","source":["generator_int8_ov = pipeline(\"text-generation\", model=model, tokenizer=tokenizer)"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1749730446354,"user_tz":240,"elapsed":11,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"b06fcf78-15d0-4db4-e697-d877c5f9fdf3","id":"TpB-JQX58AFk"},"execution_count":16,"outputs":[{"output_type":"stream","name":"stderr","text":["Device set to use cpu\n"]}]},{"cell_type":"markdown","source":["## Load Model: TheBloke/TinyLlama-1.1B-Chat-v1.0-GGUF"],"metadata":{"id":"4uReKoG49elK"}},{"cell_type":"code","source":["from llama_cpp import Llama"],"metadata":{"id":"FeVNVGeu9rex","executionInfo":{"status":"ok","timestamp":1749730450437,"user_tz":240,"elapsed":36,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":17,"outputs":[]},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/dev/.models/tinyllama-1.1b-chat-v1.0.Q2_K.gguf\"\n","generator_q2_k = Llama(\n","    model_path=model_path,\n","    n_ctx=2048,\n","    n_threads=2,\n","    verbose=False,\n",")"],"metadata":{"executionInfo":{"status":"ok","timestamp":1749735777969,"user_tz":240,"elapsed":1284,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"id":"lCSEA3i699N3"},"execution_count":66,"outputs":[]},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/dev/.models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n","generator_q4_k_m = Llama(\n","    model_path=model_path,\n","    n_ctx=2048,\n","    n_threads=2,\n","    verbose=False,\n",")"],"metadata":{"id":"u_V5LFII9rhI","executionInfo":{"status":"ok","timestamp":1749735770359,"user_tz":240,"elapsed":3110,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":65,"outputs":[]},{"cell_type":"code","source":["model_path = \"/content/drive/MyDrive/dev/.models/tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"\n","generator_q6_k = Llama(\n","    model_path=model_path,\n","    n_ctx=2048,\n","    n_threads=2,\n","    verbose=False,\n",")"],"metadata":{"id":"YK1EZOydZ_ic","executionInfo":{"status":"ok","timestamp":1749736272501,"user_tz":240,"elapsed":9733,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":77,"outputs":[]},{"cell_type":"markdown","source":["## Results"],"metadata":{"id":"IJNG6zH19Fhv"}},{"cell_type":"code","source":["import time"],"metadata":{"id":"AO5Nijbc7nSf","executionInfo":{"status":"ok","timestamp":1749730469197,"user_tz":240,"elapsed":12,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":20,"outputs":[]},{"cell_type":"code","source":["generators = {\n","    'quantized_onnx': {'func': generator_quantized_onnx, 'max_tokens': 'max_new_tokens'},\n","    'int8_ov': {'func': generator_int8_ov, 'max_tokens': 'max_new_tokens'},\n","    'q2_k': {'func': generator_q2_k, 'max_tokens': 'max_tokens', 'stop': 'stop'},\n","    'q4_k_m': {'func': generator_q4_k_m, 'max_tokens': 'max_tokens', 'stop': 'stop'},\n","    'q6_k': {'func': generator_q6_k, 'max_tokens': 'max_tokens', 'stop': 'stop'}\n","}"],"metadata":{"id":"hkEqPKveAJaC","executionInfo":{"status":"ok","timestamp":1749736360525,"user_tz":240,"elapsed":5,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":85,"outputs":[]},{"cell_type":"code","source":["def time_execution(generator, prompt, params):\n","    start_time = time.time()\n","    response = generator(prompt, **params)\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","    return {'response': response, 'elapsed_time': elapsed_time}"],"metadata":{"id":"2W-THZweH7QT","executionInfo":{"status":"ok","timestamp":1749736299992,"user_tz":240,"elapsed":4,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":79,"outputs":[]},{"cell_type":"code","source":["prompt = \"1 2 3 4 5\"\n","max_tokens = 100\n","stop = [\"\\n\"]"],"metadata":{"id":"iJI6SrQtCXE3","executionInfo":{"status":"ok","timestamp":1749736300671,"user_tz":240,"elapsed":9,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":80,"outputs":[]},{"cell_type":"code","source":["results = {}\n","for key in generators:\n","    params = {\n","        generators[key]['max_tokens']: max_tokens\n","    }\n","    if key=='q2_k' or key=='q4_k_m':\n","        params['stop'] = stop\n","    print('Running ',key)\n","    results[key] = time_execution(generators[key]['func'], prompt, params)\n"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"CGGcCdwqGSZg","executionInfo":{"status":"ok","timestamp":1749736394638,"user_tz":240,"elapsed":32120,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"704cec60-322e-4449-f968-2442d7c80395"},"execution_count":86,"outputs":[{"output_type":"stream","name":"stdout","text":["Running  quantized_onnx\n","Running  int8_ov\n","Running  q2_k\n","Running  q4_k_m\n","Running  q6_k\n"]}]},{"cell_type":"code","source":["for key in results:\n","    print(key,': ',results[key]['elapsed_time'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"bTpmQR3iLJcC","executionInfo":{"status":"ok","timestamp":1749736394650,"user_tz":240,"elapsed":8,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"62b8eec5-a03b-4042-b24c-df66d5e21923"},"execution_count":87,"outputs":[{"output_type":"stream","name":"stdout","text":["quantized_onnx :  7.08148193359375\n","int8_ov :  8.38093113899231\n","q2_k :  2.451695680618286\n","q4_k_m :  6.073390245437622\n","q6_k :  8.125742197036743\n"]}]},{"cell_type":"code","source":["print('quantized_onnx: ',results['quantized_onnx']['response'][0]['generated_text'])\n","print('int8_ov: ', results['int8_ov']['response'][0]['generated_text'])\n","print('q2_k: ', results['q2_k']['response']['choices'][0]['text'])\n","print('q4_k_m: ', results['q4_k_m']['response']['choices'][0]['text'])\n","print('q6_k: ', results['q6_k']['response']['choices'][0]['text'])"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"HajVuuQOJL5w","executionInfo":{"status":"ok","timestamp":1749736394661,"user_tz":240,"elapsed":4,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"72517fba-cf48-4f9f-ff90-100e778e78a0"},"execution_count":88,"outputs":[{"output_type":"stream","name":"stdout","text":["quantized_onnx:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 4\n","int8_ov:  1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 4\n","q2_k:   6 7 8 9 10 11 12 13 14 15 16 17 18 19 20\n","q4_k_m:   6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 4\n","q6_k:   6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 4\n"]}]},{"cell_type":"code","source":[],"metadata":{"id":"sLRWvei4JL8Q"},"execution_count":null,"outputs":[]}]}