{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"machine_shape":"hm","gpuType":"A100","collapsed_sections":["MyJaRSi44ivZ","aJ69Cse0X9_T","K_zEpzYSX5hg"],"authorship_tag":"ABX9TyP2MICk45A3809NU5PzIP7L"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","source":["* This Notebook tests the capabilities of TinyLlama quantized models to generate raw token sequences given an input sequence\n","\n","* In order to speed up the model loading, the models have been stored in a local drive\n","\n","* The following Gemini thread has been used to help build this up both conceptually and in practice: https://g.co/gemini/share/c5212fd78fad"],"metadata":{"id":"1GJAFVeVbcCn"}},{"cell_type":"code","source":["!nvcc --version"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"t1CYgHnHj04a","executionInfo":{"status":"ok","timestamp":1751817077614,"user_tz":240,"elapsed":118,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"fc082343-04dd-4f9b-c200-9a92447529c4"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["nvcc: NVIDIA (R) Cuda compiler driver\n","Copyright (c) 2005-2024 NVIDIA Corporation\n","Built on Thu_Jun__6_02:18:23_PDT_2024\n","Cuda compilation tools, release 12.5, V12.5.82\n","Build cuda_12.5.r12.5/compiler.34385749_0\n"]}]},{"cell_type":"code","source":["!nvidia-smi"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1_CoVdixj06w","executionInfo":{"status":"ok","timestamp":1751817077824,"user_tz":240,"elapsed":200,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"56f7ae7d-19b5-40c4-a3af-e8a440b44c30"},"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Jul  6 15:51:17 2025       \n","+-----------------------------------------------------------------------------------------+\n","| NVIDIA-SMI 550.54.15              Driver Version: 550.54.15      CUDA Version: 12.4     |\n","|-----------------------------------------+------------------------+----------------------+\n","| GPU  Name                 Persistence-M | Bus-Id          Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp   Perf          Pwr:Usage/Cap |           Memory-Usage | GPU-Util  Compute M. |\n","|                                         |                        |               MIG M. |\n","|=========================================+========================+======================|\n","|   0  NVIDIA A100-SXM4-40GB          Off |   00000000:00:04.0 Off |                    0 |\n","| N/A   30C    P0             43W /  400W |       0MiB /  40960MiB |      0%      Default |\n","|                                         |                        |             Disabled |\n","+-----------------------------------------+------------------------+----------------------+\n","                                                                                         \n","+-----------------------------------------------------------------------------------------+\n","| Processes:                                                                              |\n","|  GPU   GI   CI        PID   Type   Process name                              GPU Memory |\n","|        ID   ID                                                               Usage      |\n","|=========================================================================================|\n","|  No running processes found                                                             |\n","+-----------------------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"7wxnJ_0lbpRI","executionInfo":{"status":"ok","timestamp":1751817099630,"user_tz":240,"elapsed":18624,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"1e58b457-614f-4a99-d805-7c7259dc040a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive/\n"]}],"source":["import os\n","\n","# Mount Google Drive\n","from google.colab import drive\n","drive.mount('/content/drive/',force_remount=True)\n","\n","path = '/content/drive/MyDrive/dev/'"]},{"cell_type":"markdown","source":["# setup"],"metadata":{"id":"MyJaRSi44ivZ"}},{"cell_type":"markdown","source":["I struggled with the installation witb GPU support, only able to fix from https://gemini.google.com/app/f3ec74b59f92f56f"],"metadata":{"id":"jv4-ObJ0vNAY"}},{"cell_type":"code","source":["# Install the pre-built wheel for CUDA 12.4\n","!pip uninstall -y llama-cpp-python\n","!pip install llama-cpp-python --extra-index-url https://abetlen.github.io/llama-cpp-python/whl/cu124"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_4vo-v8YGlU6","executionInfo":{"status":"ok","timestamp":1751817179466,"user_tz":240,"elapsed":76071,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"4db6c658-9603-41c7-b87e-1ae6bd92cb27"},"execution_count":6,"outputs":[{"output_type":"stream","name":"stdout","text":["\u001b[33mWARNING: Skipping llama-cpp-python as it is not installed.\u001b[0m\u001b[33m\n","\u001b[0mLooking in indexes: https://pypi.org/simple, https://abetlen.github.io/llama-cpp-python/whl/cu124\n","Collecting llama-cpp-python\n","  Downloading https://github.com/abetlen/llama-cpp-python/releases/download/v0.3.12-cu124/llama_cpp_python-0.3.12-cp311-cp311-linux_x86_64.whl (504.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m504.6/504.6 MB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=4.5.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (4.14.0)\n","Requirement already satisfied: numpy>=1.20.0 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (2.0.2)\n","Collecting diskcache>=5.6.1 (from llama-cpp-python)\n","  Downloading diskcache-5.6.3-py3-none-any.whl.metadata (20 kB)\n","Requirement already satisfied: jinja2>=2.11.3 in /usr/local/lib/python3.11/dist-packages (from llama-cpp-python) (3.1.6)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2>=2.11.3->llama-cpp-python) (3.0.2)\n","Downloading diskcache-5.6.3-py3-none-any.whl (45 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m45.5/45.5 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: diskcache, llama-cpp-python\n","Successfully installed diskcache-5.6.3 llama-cpp-python-0.3.12\n"]}]},{"cell_type":"code","source":["import torch\n","import time\n","import numpy as np\n","import math\n","from llama_cpp import Llama, LlamaGrammar"],"metadata":{"id":"RBkQaTTvCPjO","executionInfo":{"status":"ok","timestamp":1751819344057,"user_tz":240,"elapsed":4,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":62,"outputs":[]},{"cell_type":"code","source":["print(f\"CUDA available: {torch.cuda.is_available()}\")\n","print(f\"CUDA version: {torch.version.cuda}\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"SBKtnl0ALvvS","executionInfo":{"status":"ok","timestamp":1751817181272,"user_tz":240,"elapsed":11,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"5ab40299-2c05-4951-987e-4a811248f4ab"},"execution_count":8,"outputs":[{"output_type":"stream","name":"stdout","text":["CUDA available: True\n","CUDA version: 12.4\n"]}]},{"cell_type":"markdown","source":["# models"],"metadata":{"id":"l-Yi5NrIcG_Z"}},{"cell_type":"markdown","source":["## n_gpu_layers"],"metadata":{"id":"aJ69Cse0X9_T"}},{"cell_type":"markdown","source":["* This Notebook is just to test running various quantized TinyLlama models\n","\n","* As a result, we will either use either n_gpu_layers = 0 (for running on CPU only) or n_gpu_layers = 33 (33 GPU layers)\n","\n","* Refer to the Gemini explanation below in order to optimize n_gpu_layers when running the code for getting results"],"metadata":{"id":"grecOm2eYMPe"}},{"cell_type":"code","source":["# Set\n","n_gpu_layers = 0\n","#n_gpu_layers = 33"],"metadata":{"id":"QoMjMcI6Wv9s"},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":["https://g.co/gemini/share/1de12d982e5f\n","\n","That's an excellent and very important question for getting the best performance out of your models. Setting `n_gpu_layers` is the primary way you control the balance between CPU and GPU work.\n","\n","Here’s a breakdown of what it means and how to find the optimal value for your specific system.\n","\n","### What `n_gpu_layers` Does\n","\n","This parameter determines how many layers of the neural network are \"offloaded\" to your GPU's VRAM.\n","\n","  * **CPU:** Your computer's main processor. It has access to a large amount of slower system RAM.\n","  * **GPU:** Your graphics card. It has a smaller amount of very fast video RAM (VRAM) and is designed for the massive parallel calculations that LLMs require.\n","\n","**The trade-off is simple:**\n","\n","  * **More layers on the GPU (`n_gpu_layers` is high):** Inference (generating text) is much faster.\n","  * **Fewer layers on the GPU (`n_gpu_layers` is low):** More work is done on the slower CPU, but it uses less of your precious VRAM.\n","\n","The \"optimal\" value is therefore: **The maximum number of layers you can fit into your GPU's VRAM without causing errors, while leaving a small buffer for your operating system.**\n","\n","-----\n","\n","### How to Find the Optimal Value: A Practical Guide\n","\n","The best way to find the perfect number is through a bit of experimentation. Here is a simple, effective process:\n","\n","#### Step 1: Know Your Total VRAM\n","\n","First, check how much VRAM your GPU has.\n","\n","  * **On Windows:** Open the Task Manager, go to the \"Performance\" tab, and click on your GPU. Look for \"Dedicated GPU Memory\".\n","  * **On Linux (with NVIDIA):** Open a terminal and run the command `nvidia-smi`. It will show you your GPU's total memory (e.g., `8192MiB`).\n","\n","#### Step 2: Start High and Let the Model Tell You\n","\n","You don't need to guess the number of layers. The easiest method is to set `n_gpu_layers` to a very high number. The `llama-cpp-python` library is smart enough to only offload as many layers as it can actually fit.\n","\n","Modify your model loading code like this:\n","\n","```python\n","from llama_cpp import Llama\n","\n","llm = Llama(\n","    model_path=\"/path/to/your/model.gguf\",\n","    # Set a very high number to offload as many layers as possible\n","    n_gpu_layers=999,  # <--- START WITH A HIGH NUMBER\n","    n_ctx=2048,\n","    verbose=True  # <--- Set to True to see the loading output\n",")\n","```\n","\n","#### Step 3: Observe the Output\n","\n","When you run this code, `llama.cpp` will print detailed information as it loads the model. Look for a line that looks like this:\n","\n","```\n","llama_model_load_internal: offloaded 33 of 33 layers to GPU\n","```\n","\n","or\n","\n","```\n","llm_load_tensors: offloaded 33/33 layers to GPU\n","```\n","\n","**That number is your answer.** In this example, the model has 33 layers, and all of them fit into your VRAM. Your optimal `n_gpu_layers` is **33**. You can then set `n_gpu_layers=33` in your code for future runs to be precise.\n","\n","#### What If It Crashes?\n","\n","If you set the number too high manually and your system becomes unstable or the program crashes with a \"CUDA out of memory\" error, it means you've exceeded your VRAM. In that case, simply lower the number and try again. For example, if `n_gpu_layers=33` crashes, try `30`, then `28`, until you find a stable number.\n","\n","**Pro Tip:** Often, the last one or two layers of a model can be disproportionately large. If you find that offloading all layers works but your computer feels sluggish, try reducing the number by 1 or 2 (e.g., `n_gpu_layers=31` instead of `33`). This leaves a bit more VRAM for your operating system and can lead to a smoother overall experience with a negligible impact on performance.\n","That's an excellent and very practical question. Finding the optimal `n_gpu_layers` is the key to getting the best performance out of your hardware. Here’s a breakdown of what it means and how to find the best value for your specific setup.\n","\n","### What `n_gpu_layers` Does\n","\n","This parameter controls how many layers of the model are \"offloaded\" from your system's RAM to the GPU's dedicated, high-speed VRAM.\n","\n","  * **`n_gpu_layers = 0`**: The entire model runs on your CPU. This is the slowest option but uses the least VRAM.\n","  * **`n_gpu_layers > 0`**: The specified number of layers are moved to the GPU. Since GPUs are massively parallel, they can process these layers much faster than a CPU, leading to a significant speedup in generating tokens.\n","  * **The Goal**: Offload as many layers as possible to the GPU without running out of VRAM.\n","\n","### The Trade-Off: Speed vs. VRAM\n","\n","The main limiting factor is your GPU's VRAM capacity. Each layer you offload consumes a chunk of VRAM. If you try to offload more layers than your VRAM can hold, the program will crash with an out-of-memory error.\n","\n","The performance gain is not linear. The bulk of the computation happens in the middle layers. Offloading the first few layers gives a huge speed boost. Continuing to offload more layers gives diminishing returns, but is still faster.\n","\n","### How to Find the Optimal Value: A Simple Experiment\n","\n","The best way to find the ideal number is to test it empirically.\n","\n","**Step 1: Start High**\n","\n","The easiest way to start is to set `n_gpu_layers` to a very large number, like `-1` or `999`. The `llama-cpp-python` library is smart enough to know this means \"offload as many layers as you possibly can\".\n","\n","```python\n","# Try to offload all layers\n","llm = Llama(\n","    model_path=\"path/to/your/model.gguf\",\n","    n_gpu_layers=-1, # -1 means \"all possible layers\"\n","    verbose=True # Set to True to see the loading output\n",")\n","```\n","\n","**Step 2: Watch the Output During Loading**\n","\n","When the model loads with `verbose=True`, it will print detailed information. Look for a line that looks like this:\n","\n","`llama_model_load_internal: offloaded 33 of 33 layers to GPU`\n","\n","This tells you the maximum number of layers the model has (`33` in this case) and how many it successfully moved to the GPU. **The number it successfully offloaded is your practical maximum.**\n","\n","**Step 3: Monitor Your VRAM**\n","\n","While the model is loaded, open your system's GPU monitoring tool:\n","\n","  * **NVIDIA GPUs**: Use `nvidia-smi` in your command line/terminal.\n","  * **AMD GPUs**: Use `radeontop` or the monitoring utility in your driver software.\n","  * **Windows**: The Performance tab in the Task Manager (select your GPU).\n","\n","Check how much VRAM is being used. If it's very close to the maximum (e.g., 7.8GB / 8.0GB), you are at the limit.\n","\n","**Step 4: Fine-Tune if Necessary**\n","\n","  * **If it crashes with an \"out of memory\" error:** Your GPU can't handle all the layers. Reduce the number. If the model has 33 layers, try `n_gpu_layers=28`, then `25`, and so on, until it loads successfully.\n","  * **If it loads but VRAM is maxed out:** You've found a good value. You might want to reduce it by 1 or 2 layers (`n_gpu_layers=31` instead of `33`) just to leave a little breathing room for your operating system or other applications, which can prevent stuttering.\n","\n","**In summary, the ideal `n_gpu_layers` is the highest number you can set without running out of VRAM.** The \"set it to -1 and see what happens\" method is the quickest way to find that number.\n"],"metadata":{"id":"Io9oRqGkYBqX"}},{"cell_type":"markdown","source":["## models"],"metadata":{"id":"K_zEpzYSX5hg"}},{"cell_type":"markdown","source":["https://g.co/gemini/share/5e6bc8594d7d\n","\n"],"metadata":{"id":"807bsTmVZ91H"}},{"cell_type":"code","source":["model_path = path+\".models/tinyllama-1.1b-chat-v1.0.Q2_K.gguf\"\n","generator_q2_k = Llama(\n","    model_path=model_path,\n","    chat_format=\"disabled\",  # This prevents the model from wrapping input in chat templates\n","    n_ctx=2048,\n","    n_threads=2,\n","    n_gpu_layers=n_gpu_layers,\n","    logits_all=True,\n","    verbose=False,\n",")"],"metadata":{"id":"CK2dMnRlAovw","executionInfo":{"status":"ok","timestamp":1751817819385,"user_tz":240,"elapsed":524,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"62a7bd72-f6d2-4499-fdf4-06c5cae2ee0e"},"execution_count":38,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"]}]},{"cell_type":"code","source":["# Check the number of layers offloaded to the GPU\n","offloaded_layers = generator_q2_k.model_params.n_gpu_layers\n","print(f\"✅ Successfully offloaded {offloaded_layers} layers to the GPU\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"LgJ6G56DB7XL","executionInfo":{"status":"ok","timestamp":1751817819397,"user_tz":240,"elapsed":13,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"7fc175e6-31c1-4e01-92fa-93da30fee601"},"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Successfully offloaded 33 layers to the GPU\n"]}]},{"cell_type":"code","source":["model_path = path+\".models/tinyllama-1.1b-chat-v1.0.Q4_K_M.gguf\"\n","generator_q4_k_m = Llama(\n","    model_path=model_path,\n","    chat_format=\"disabled\",  # This prevents the model from wrapping input in chat templates\n","    n_ctx=2048,\n","    n_threads=2,\n","    n_gpu_layers=n_gpu_layers,\n","    logits_all=True,\n","    verbose=False,\n",")"],"metadata":{"id":"u_V5LFII9rhI","executionInfo":{"status":"ok","timestamp":1751817820040,"user_tz":240,"elapsed":642,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"924ee17a-b155-420d-a864-1d4b26c98c71"},"execution_count":40,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"]}]},{"cell_type":"code","source":["# Check the number of layers offloaded to the GPU\n","offloaded_layers = generator_q4_k_m.model_params.n_gpu_layers\n","print(f\"✅ Successfully offloaded {offloaded_layers} layers to the GPU\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"eO-rWZvZIeJl","executionInfo":{"status":"ok","timestamp":1751817820055,"user_tz":240,"elapsed":13,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"c6c5a9b1-4936-41c9-e9e9-5a46bf7471b1"},"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Successfully offloaded 33 layers to the GPU\n"]}]},{"cell_type":"code","source":["model_path = path+\".models/tinyllama-1.1b-chat-v1.0.Q6_K.gguf\"\n","generator_q6_k = Llama(\n","    model_path=model_path,\n","    chat_format=\"disabled\",  # This prevents the model from wrapping input in chat templates\n","    n_ctx=2048,\n","    n_threads=2,\n","    n_gpu_layers=n_gpu_layers,\n","    logits_all=True,\n","    verbose=False,\n",")"],"metadata":{"id":"YK1EZOydZ_ic","executionInfo":{"status":"ok","timestamp":1751817820890,"user_tz":240,"elapsed":834,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"colab":{"base_uri":"https://localhost:8080/"},"outputId":"95bf408a-7e46-4978-9d19-3fd2d941db33"},"execution_count":42,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"]}]},{"cell_type":"code","source":["# Check the number of layers offloaded to the GPU\n","offloaded_layers = generator_q6_k.model_params.n_gpu_layers\n","print(f\"✅ Successfully offloaded {offloaded_layers} layers to the GPU\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"pLUr7_xjIp6X","executionInfo":{"status":"ok","timestamp":1751817820906,"user_tz":240,"elapsed":12,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"e367bac7-4ade-4440-f48c-2dc7cdf52217"},"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Successfully offloaded 33 layers to the GPU\n"]}]},{"cell_type":"code","source":["model_path = path+\".models/tinyllama-1.1b-chat-v1.0.Q8_0.gguf\"\n","generator_q8_0 = Llama(\n","    model_path=model_path,\n","    chat_format=\"disabled\",  # This prevents the model from wrapping input in chat templates\n","    n_ctx=2048,\n","    n_threads=2,\n","    n_gpu_layers=n_gpu_layers,\n","    logits_all=True,\n","    verbose=False,\n",")"],"metadata":{"executionInfo":{"status":"ok","timestamp":1751817821934,"user_tz":240,"elapsed":1027,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"id":"a3mR-8v5PPoV","colab":{"base_uri":"https://localhost:8080/"},"outputId":"cf5e2f3e-ed51-42d6-92c9-aafadc2a7967"},"execution_count":44,"outputs":[{"output_type":"stream","name":"stderr","text":["llama_kv_cache_unified: LLAMA_SET_ROWS=0, using old ggml_cpy() method for backwards compatibility\n"]}]},{"cell_type":"code","source":["# Check the number of layers offloaded to the GPU\n","offloaded_layers = generator_q8_0.model_params.n_gpu_layers\n","print(f\"✅ Successfully offloaded {offloaded_layers} layers to the GPU\")"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1751817821944,"user_tz":240,"elapsed":6,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"708d406e-125b-4bb0-eb72-2512ec13f2d5","id":"Yb0Ns6CLPPoW"},"execution_count":45,"outputs":[{"output_type":"stream","name":"stdout","text":["✅ Successfully offloaded 33 layers to the GPU\n"]}]},{"cell_type":"markdown","source":["# tests"],"metadata":{"id":"Np4sNt4I4aN_"}},{"cell_type":"code","source":["# Define the grammar in GBNF format\n","# This grammar allows for a sequence of numbers (integers or decimals) separated by commas\n","# https://g.co/gemini/share/938d5c086f74\n","\n","gbnf_grammar_str = r'''\n","root   ::= sequence\n","sequence ::= number (\",\" number)*\n","number ::= (\"-\")? ([0-9]+ | [0-9]+ \".\" [0-9]+)\n","'''\n","try:\n","    grammar = LlamaGrammar.from_string(gbnf_grammar_str)\n","    print(\"Strict grammar parsed successfully\")\n","    print(grammar)\n","except Exception as e:\n","    print(f\"Error parsing grammar: {e}\")\n","    grammar = None"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"80ofIhhNkDJB","executionInfo":{"status":"ok","timestamp":1751820731469,"user_tz":240,"elapsed":9,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"42ecf88c-57f7-445c-fe1b-bc3009c76503"},"execution_count":109,"outputs":[{"output_type":"stream","name":"stdout","text":["Strict grammar parsed successfully.\n"]}]},{"cell_type":"code","source":["# Define the prompt with number sequence\n","\n","prompt = \"1.0,2.1,3.3,4.2,5.7,\"\n","#prompt = \"1,2,3,4,5,\""],"metadata":{"id":"nnfAP-KuZTYq"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":["# Define parameters for running the model\n","# https://g.co/gemini/share/5e0aed5cf4b8\n","# https://g.co/gemini/share/8e15743e2cd6\n","# https://g.co/gemini/share/1dcee2a72019\n","\n","parameters = {\n","    'max_tokens': 20,\n","    'logprobs': 10,\n","    'grammar': grammar,\n","    'temperature': 0.9\n","}"],"metadata":{"id":"mX2kxkYKMCA0","executionInfo":{"status":"ok","timestamp":1751820754529,"user_tz":240,"elapsed":4,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":113,"outputs":[]},{"cell_type":"code","source":["generators = {\n","    'q2_k': generator_q2_k,\n","    'q4_k_m': generator_q4_k_m,\n","    'q6_k': generator_q6_k,\n","    'q8_0': generator_q8_0,\n","}"],"metadata":{"id":"hkEqPKveAJaC","executionInfo":{"status":"ok","timestamp":1751819995557,"user_tz":240,"elapsed":5,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":76,"outputs":[]},{"cell_type":"code","source":["# Generate output sequence and logprobs\n","\n","def time_execution(generator, prompt, params):\n","\n","    start_time = time.time()\n","    response = generator(prompt, **params)\n","    end_time = time.time()\n","    elapsed_time = end_time - start_time\n","\n","    choice = response['choices'][0]\n","    text = choice['text']\n","    logprobs_data = choice['logprobs']\n","\n","    return {'response': text,\n","            'logprobs_data': logprobs_data,\n","            'elapsed_time': elapsed_time}"],"metadata":{"id":"2W-THZweH7QT","executionInfo":{"status":"ok","timestamp":1751819996954,"user_tz":240,"elapsed":13,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}}},"execution_count":77,"outputs":[]},{"cell_type":"code","source":["# Run results for all models in dict generators\n","\n","results = {}\n","r = []\n","t = []\n","\n","for key in generators:\n","    results[key] = {}\n","    print(f'### Running {key}')\n","    results[key] = time_execution(generators[key], prompt, parameters)\n","    s = results[key]['response']\n","    print(f'\\tresponse = {s}')\n","    s = results[key]['logprobs_data']\n","    print(f'\\tlogprobs_data = {s}')\n","    s = results[key]['elapsed_time']\n","    print(f'\\telapsed_time = {s}')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"IpwYiIk2PQ11","executionInfo":{"status":"ok","timestamp":1751820759980,"user_tz":240,"elapsed":3050,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"ba7bedb0-6ac3-4680-e49d-9f6e1869c6a0"},"execution_count":114,"outputs":[{"output_type":"stream","name":"stdout","text":["### Running q2_k\n","\tresponse = 6.2,7.4,8.9,10.4,12.\n","\telapsed_time = {'tokens': ['6', '.', '2', ',', '7', '.', '4', ',', '8', '.', '9', ',', '1', '0', '.', '4', ',', '1', '2', '.'], 'text_offset': [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], 'token_logprobs': [np.float32(-0.51337457), np.float32(-0.048911024), np.float32(-2.2620232), np.float32(-0.32777563), np.float32(-0.2769009), np.float32(-0.052663155), np.float32(-2.0914621), np.float32(-0.2954199), np.float32(-0.16116016), np.float32(-0.031512357), np.float32(-2.78941), np.float32(-0.25423846), np.float32(-0.31939754), np.float32(-0.059035994), np.float32(-0.122858405), np.float32(-2.2547197), np.float32(-0.79676276), np.float32(-0.13400947), np.float32(-0.2971776), np.float32(-0.05908758)], 'top_logprobs': [{'6': np.float32(-0.51337457), '7': np.float32(-1.759495), '8': np.float32(-2.716346), '1': np.float32(-3.0023444), '5': np.float32(-3.2694342), '9': np.float32(-4.275054), '\\n': np.float32(-5.0873775), '2': np.float32(-5.150531), '4': np.float32(-5.433572), ' ': np.float32(-5.465583)}, {'.': np.float32(-0.048911024), ',': np.float32(-3.6438055), '\\n': np.float32(-5.32397), ')': np.float32(-6.297783), ' ': np.float32(-6.5362673), '(': np.float32(-7.174039), '-': np.float32(-7.22937), '4': np.float32(-7.3909445), '0': np.float32(-7.522237), ' (': np.float32(-7.525702)}, {'5': np.float32(-1.8870938), '7': np.float32(-1.9605668), '6': np.float32(-1.9753602), '8': np.float32(-2.212332), '2': np.float32(-2.2620232), '4': np.float32(-2.4341176), '1': np.float32(-2.494696), '3': np.float32(-2.5476735), '9': np.float32(-2.561536), '0': np.float32(-3.620865)}, {',': np.float32(-0.32777563), '\\n': np.float32(-2.5254476), ' ': np.float32(-3.2419813), '.': np.float32(-3.4509141), ')': np.float32(-3.8901231), '5': np.float32(-4.857654), ']': np.float32(-4.988548), ' (': np.float32(-5.5140595), ').': np.float32(-5.638017), ';': np.float32(-6.021051)}, {'7': np.float32(-0.2769009), '6': np.float32(-2.015943), '8': np.float32(-2.903995), '1': np.float32(-4.6415763), '9': np.float32(-4.7208896), '\\n': np.float32(-5.1295295), '5': np.float32(-5.264501), ' ': np.float32(-5.4149194), '0': np.float32(-6.2448726), '4': np.float32(-6.4596667)}, {'.': np.float32(-0.052663155), ',': np.float32(-3.506511), '\\n': np.float32(-5.278149), ')': np.float32(-5.552997), ' ': np.float32(-6.6107793), ']': np.float32(-7.0703874), '-': np.float32(-7.7643538), ').': np.float32(-8.171729), '0': np.float32(-8.296573), '),': np.float32(-8.499761)}, {'1': np.float32(-1.7681065), '2': np.float32(-1.8975067), '3': np.float32(-1.9193039), '4': np.float32(-2.0914621), '5': np.float32(-2.182952), '7': np.float32(-2.497366), '0': np.float32(-2.698556), '6': np.float32(-2.7767859), '8': np.float32(-2.92163), '9': np.float32(-3.5408554)}, {',': np.float32(-0.2954199), '\\n': np.float32(-2.7062867), '.': np.float32(-3.301246), ' ': np.float32(-3.6132977), ')': np.float32(-3.7039616), ']': np.float32(-4.8320255), ').': np.float32(-5.3449345), ';': np.float32(-6.0108643), '5': np.float32(-6.0766964), '  ': np.float32(-6.1546288)}, {'8': np.float32(-0.16116016), '7': np.float32(-2.5977666), '9': np.float32(-3.3020065), '1': np.float32(-4.4116826), '6': np.float32(-4.86431), '\\n': np.float32(-5.845778), ' ': np.float32(-5.857054), '5': np.float32(-6.4220986), '0': np.float32(-6.960772), '2': np.float32(-7.2510333)}, {'.': np.float32(-0.031512357), ',': np.float32(-4.0228453), '\\n': np.float32(-5.597396), ')': np.float32(-6.6219254), ' ': np.float32(-7.0891905), ':': np.float32(-7.858417), '0': np.float32(-8.071811), '5': np.float32(-8.545182), ']': np.float32(-8.64494), '4': np.float32(-8.712808)}, {'5': np.float32(-1.9230455), '3': np.float32(-2.0765681), '4': np.float32(-2.0840478), '6': np.float32(-2.2040534), '2': np.float32(-2.2160459), '7': np.float32(-2.269051), '1': np.float32(-2.5070338), '8': np.float32(-2.5826383), '9': np.float32(-2.78941), '0': np.float32(-2.7913423)}, {',': np.float32(-0.25423846), '\\n': np.float32(-2.9375346), ' ': np.float32(-3.243687), '.': np.float32(-3.3851607), ')': np.float32(-4.2499757), ']': np.float32(-5.3220468), '9': np.float32(-5.466842), ' (': np.float32(-5.6425557), ').': np.float32(-5.886029), ';': np.float32(-5.9980593)}, {'1': np.float32(-0.31939754), '9': np.float32(-1.4630591), '8': np.float32(-4.6336703), '7': np.float32(-5.646107), '\\n': np.float32(-5.655537), ' ': np.float32(-5.6846986), '6': np.float32(-5.7069964), '2': np.float32(-5.829266), '0': np.float32(-5.8718896), '5': np.float32(-6.4259896)}, {'0': np.float32(-0.059035994), '1': np.float32(-3.425413), '2': np.float32(-4.707851), '.': np.float32(-4.94939), '3': np.float32(-6.0525546), '5': np.float32(-6.577728), '6': np.float32(-6.8319983), '4': np.float32(-6.912706), '9': np.float32(-7.279073), '8': np.float32(-7.3140273)}, {'.': np.float32(-0.122858405), ',': np.float32(-3.324575), '\\n': np.float32(-4.1016383), '0': np.float32(-4.4670453), ')': np.float32(-4.630409), ' ': np.float32(-4.989491), ').': np.float32(-6.3191247), ' (': np.float32(-6.3682446), ']': np.float32(-6.536817), ':': np.float32(-6.6727347)}, {'0': np.float32(-1.8450108), '1': np.float32(-2.1322079), '5': np.float32(-2.1571121), '2': np.float32(-2.2125673), '3': np.float32(-2.2282848), '4': np.float32(-2.2547197), '6': np.float32(-2.4331074), '7': np.float32(-2.4971218), '8': np.float32(-2.8688002), '9': np.float32(-3.0421333)}, {',': np.float32(-0.79676276), '\\n': np.float32(-1.9917443), '.': np.float32(-2.6688797), ' ': np.float32(-2.7114441), ')': np.float32(-2.9268029), ']': np.float32(-4.4154286), ' (': np.float32(-4.469917), ').': np.float32(-4.642706), ';': np.float32(-5.077995), ':': np.float32(-5.593516)}, {'1': np.float32(-0.13400947), '2': np.float32(-3.2933817), '\\n': np.float32(-4.529229), ' etc': np.float32(-5.0217066), '0': np.float32(-5.3168306), ' ': np.float32(-5.4001584), '3': np.float32(-5.874258), ' and': np.float32(-5.9115963), 'etc': np.float32(-6.057568), '∞': np.float32(-6.561822)}, {'2': np.float32(-0.2971776), '1': np.float32(-2.088051), '3': np.float32(-2.8698142), '5': np.float32(-3.046322), '4': np.float32(-4.4572883), '0': np.float32(-4.8500237), '6': np.float32(-5.593593), '.': np.float32(-6.094483), '7': np.float32(-6.487354), '8': np.float32(-7.220794)}, {'.': np.float32(-0.05908758), ',': np.float32(-3.3618667), '\\n': np.float32(-4.9710093), ')': np.float32(-6.4175406), ' ': np.float32(-6.5095224), ']': np.float32(-6.714164), ' (': np.float32(-7.820938), '-': np.float32(-8.151901), ':': np.float32(-8.184379), '+': np.float32(-8.189781)}]}\n","### Running q4_k_m\n","\tresponse = 6.4,7.0,7.4,8.3,9.5,\n","\telapsed_time = {'tokens': ['6', '.', '4', ',', '7', '.', '0', ',', '7', '.', '4', ',', '8', '.', '3', ',', '9', '.', '5', ','], 'text_offset': [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], 'token_logprobs': [np.float32(-0.15679584), np.float32(-0.008508148), np.float32(-2.229335), np.float32(-0.27171114), np.float32(-0.09804924), np.float32(-0.007328769), np.float32(-1.9266868), np.float32(-0.31733972), np.float32(-1.4492874), np.float32(-0.0029207687), np.float32(-2.8812988), np.float32(-0.20391878), np.float32(-0.5574627), np.float32(-0.0030169229), np.float32(-2.4267988), np.float32(-0.21482651), np.float32(-0.50563145), np.float32(-0.0059452597), np.float32(-2.3013258), np.float32(-0.26886827)], 'top_logprobs': [{'6': np.float32(-0.15679584), '7': np.float32(-3.0627892), '5': np.float32(-3.932974), '8': np.float32(-4.4240136), '1': np.float32(-4.58796), ' ': np.float32(-5.2318807), '9': np.float32(-5.438828), '2': np.float32(-5.497339), '\\n': np.float32(-5.56045), '4': np.float32(-5.690743)}, {'.': np.float32(-0.008508148), ',': np.float32(-5.936585), '\\n': np.float32(-6.8939815), ':': np.float32(-7.661301), ' ': np.float32(-8.591951), '-': np.float32(-8.648585), '': np.float32(-8.956575), ')': np.float32(-9.086986), ' (': np.float32(-9.1829815), ';': np.float32(-9.244083)}, {'5': np.float32(-1.9048188), '6': np.float32(-2.1661608), '8': np.float32(-2.1881669), '9': np.float32(-2.2059338), '4': np.float32(-2.229335), '1': np.float32(-2.262912), '3': np.float32(-2.4412072), '2': np.float32(-2.48269), '7': np.float32(-2.6750643), '0': np.float32(-2.837989)}, {',': np.float32(-0.27171114), '.': np.float32(-3.39206), '\\n': np.float32(-3.4134843), ' ': np.float32(-3.4448211), '': np.float32(-4.8689537), ':': np.float32(-4.9441967), ')': np.float32(-4.999716), ';': np.float32(-5.2299643), ' (': np.float32(-5.559857), ']': np.float32(-5.662918)}, {'7': np.float32(-0.09804924), '6': np.float32(-3.0569515), '8': np.float32(-3.8865948), ' ': np.float32(-5.7115955), '1': np.float32(-5.7427053), '9': np.float32(-5.897381), '5': np.float32(-5.902871), '\\n': np.float32(-6.1886635), '2': np.float32(-7.2579556), '4': np.float32(-7.527361)}, {'.': np.float32(-0.007328769), ',': np.float32(-5.8684406), '\\n': np.float32(-7.1922255), ':': np.float32(-7.885851), ' ': np.float32(-8.326913), ')': np.float32(-8.601528), '-': np.float32(-8.904499), '': np.float32(-9.321032), ';': np.float32(-9.5084505), ' (': np.float32(-9.7305975)}, {'1': np.float32(-1.3942313), '2': np.float32(-1.75417), '0': np.float32(-1.9266868), '3': np.float32(-2.2446465), '5': np.float32(-2.5316224), '4': np.float32(-2.8160014), '9': np.float32(-3.0423026), '6': np.float32(-3.045876), '8': np.float32(-3.0840764), '7': np.float32(-3.129909)}, {',': np.float32(-0.31733972), ' ': np.float32(-3.2589376), '\\n': np.float32(-3.3039196), '.': np.float32(-3.519468), ':': np.float32(-4.5050287), '': np.float32(-4.6344967), ')': np.float32(-4.9738584), ';': np.float32(-5.1603994), ' (': np.float32(-5.3625164), ']': np.float32(-5.616561)}, {'8': np.float32(-0.32660958), '7': np.float32(-1.4492874), '9': np.float32(-4.6476755), '6': np.float32(-4.831048), ' ': np.float32(-5.5774755), '1': np.float32(-5.6580734), '\\n': np.float32(-6.2688513), '5': np.float32(-6.351081), '2': np.float32(-6.9640727), '4': np.float32(-7.0978336)}, {'.': np.float32(-0.0029207687), ',': np.float32(-6.5630336), '\\n': np.float32(-8.965085), '-': np.float32(-9.092956), '+': np.float32(-9.129233), ':': np.float32(-9.208941), '0': np.float32(-9.406362), ' ': np.float32(-9.733779), '/': np.float32(-9.755702), 'a': np.float32(-10.4948845)}, {'5': np.float32(-1.6601703), '7': np.float32(-1.9110115), '8': np.float32(-1.9465396), '9': np.float32(-1.9573734), '6': np.float32(-2.0338209), '1': np.float32(-2.6999376), '4': np.float32(-2.8812988), '2': np.float32(-3.020213), '3': np.float32(-3.046395), '0': np.float32(-3.6123922)}, {',': np.float32(-0.20391878), '.': np.float32(-3.2219603), '\\n': np.float32(-3.3842604), ' ': np.float32(-3.8587859), '': np.float32(-5.0131297), ')': np.float32(-5.1658626), '-': np.float32(-5.464331), ';': np.float32(-5.6810107), ':': np.float32(-5.920634), ' (': np.float32(-6.1440825)}, {'8': np.float32(-0.5574627), '7': np.float32(-0.9545517), '9': np.float32(-4.1017256), '6': np.float32(-4.5552406), '1': np.float32(-5.5425663), ' ': np.float32(-6.1242905), '5': np.float32(-6.368599), '\\n': np.float32(-7.1053276), '4': np.float32(-7.2560005), '2': np.float32(-7.5133867)}, {'.': np.float32(-0.0030169229), ',': np.float32(-6.6155314), '\\n': np.float32(-8.222045), ':': np.float32(-8.850305), ' ': np.float32(-8.968251), '': np.float32(-10.137435), ')': np.float32(-10.213832), '-': np.float32(-10.377547), '0': np.float32(-10.539587), ' (': np.float32(-10.572183)}, {'0': np.float32(-1.2409431), '1': np.float32(-1.621112), '2': np.float32(-1.9238263), '3': np.float32(-2.4267988), '5': np.float32(-2.5986643), '4': np.float32(-2.858035), '6': np.float32(-3.0997744), '7': np.float32(-3.2659407), '8': np.float32(-3.3624907), '9': np.float32(-3.5566835)}, {',': np.float32(-0.21482651), '\\n': np.float32(-3.2091084), '.': np.float32(-3.360857), ' ': np.float32(-3.5809069), '': np.float32(-5.1066656), ')': np.float32(-5.4378586), ']': np.float32(-5.8229294), ';': np.float32(-5.8749905), '  ': np.float32(-5.9149265), ':': np.float32(-6.1164894)}, {'9': np.float32(-0.50563145), '8': np.float32(-1.0565977), '1': np.float32(-3.8279705), '7': np.float32(-4.6321526), ' ': np.float32(-5.6752644), '6': np.float32(-5.9437695), '\\n': np.float32(-6.425475), '5': np.float32(-6.437763), '2': np.float32(-6.992258), '4': np.float32(-7.3713636)}, {'.': np.float32(-0.0059452597), ',': np.float32(-6.1787987), '\\n': np.float32(-7.134348), ':': np.float32(-8.205353), ' ': np.float32(-8.540431), ')': np.float32(-8.547296), '-': np.float32(-9.311296), '+': np.float32(-9.514067), ' .': np.float32(-9.687484), '': np.float32(-9.777304)}, {'0': np.float32(-1.3820676), '1': np.float32(-1.9595011), '2': np.float32(-2.0929546), '5': np.float32(-2.3013258), '3': np.float32(-2.532115), '4': np.float32(-2.5362835), '7': np.float32(-2.714898), '6': np.float32(-2.741373), '8': np.float32(-2.9947166), '9': np.float32(-3.1257591)}, {',': np.float32(-0.26886827), '\\n': np.float32(-3.0208576), '.': np.float32(-3.1281402), ' ': np.float32(-3.4181), ')': np.float32(-4.7761374), '': np.float32(-5.142288), ']': np.float32(-5.543809), '  ': np.float32(-5.5546236), ':': np.float32(-5.7896476), ';': np.float32(-5.8766007)}]}\n","### Running q6_k\n","\tresponse = 6.6,7.8,9.5,11.6,13.\n","\telapsed_time = {'tokens': ['6', '.', '6', ',', '7', '.', '8', ',', '9', '.', '5', ',', '1', '1', '.', '6', ',', '1', '3', '.'], 'text_offset': [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], 'token_logprobs': [np.float32(-0.27276123), np.float32(-0.005659746), np.float32(-2.1210847), np.float32(-0.28723353), np.float32(-0.14534469), np.float32(-0.005699574), np.float32(-2.219911), np.float32(-0.46813697), np.float32(-1.8444738), np.float32(-0.023853585), np.float32(-2.535052), np.float32(-0.21892065), np.float32(-0.08255476), np.float32(-0.6781656), np.float32(-0.05614332), np.float32(-2.5068934), np.float32(-0.30217975), np.float32(-0.04613561), np.float32(-0.68167955), np.float32(-0.022560485)], 'top_logprobs': [{'6': np.float32(-0.27276123), '7': np.float32(-2.4762487), '5': np.float32(-3.2117767), '1': np.float32(-4.1530533), '8': np.float32(-4.3127146), '4': np.float32(-4.591756), ' ': np.float32(-4.7757225), '2': np.float32(-4.9030237), '3': np.float32(-5.0156364), '\\n': np.float32(-5.144185)}, {'.': np.float32(-0.005659746), ',': np.float32(-5.966247), '\\n': np.float32(-7.388331), ':': np.float32(-8.466256), ')': np.float32(-9.002795), ' (': np.float32(-9.121663), ' ': np.float32(-9.144532), '(': np.float32(-9.30378), '-': np.float32(-9.309838), ';': np.float32(-9.5907135)}, {'8': np.float32(-1.9172207), '9': np.float32(-1.9808575), '5': np.float32(-2.008346), '7': np.float32(-2.1054692), '6': np.float32(-2.1210847), '4': np.float32(-2.4141726), '3': np.float32(-2.5630946), '1': np.float32(-2.6742215), '2': np.float32(-2.6774087), '0': np.float32(-3.4060884)}, {',': np.float32(-0.28723353), '\\n': np.float32(-2.9429562), ' ': np.float32(-3.4075787), '.': np.float32(-3.7148917), ')': np.float32(-4.225916), ']': np.float32(-4.715934), ';': np.float32(-5.053419), ' (': np.float32(-5.2607822), '/': np.float32(-5.6761446), '  ': np.float32(-5.7516375)}, {'7': np.float32(-0.14534469), '8': np.float32(-2.8816986), '6': np.float32(-3.3401155), '5': np.float32(-4.989765), '1': np.float32(-5.148923), '9': np.float32(-5.168994), ' ': np.float32(-5.218648), '\\n': np.float32(-5.714505), '4': np.float32(-6.402011), '2': np.float32(-6.6574574)}, {'.': np.float32(-0.005699574), ',': np.float32(-5.8491006), '\\n': np.float32(-7.3215528), ' ': np.float32(-8.612861), ')': np.float32(-8.6193075), ':': np.float32(-8.910785), '.,': np.float32(-9.693953), ']': np.float32(-9.760956), '-': np.float32(-9.894042), ' (': np.float32(-9.924203)}, {'7': np.float32(-1.7548499), '5': np.float32(-1.7869124), '6': np.float32(-2.054502), '8': np.float32(-2.219911), '4': np.float32(-2.3049273), '9': np.float32(-2.4639602), '3': np.float32(-2.5340257), '2': np.float32(-2.737143), '1': np.float32(-2.7789521), '0': np.float32(-3.495542)}, {',': np.float32(-0.46813697), ' ': np.float32(-2.6267655), '\\n': np.float32(-2.6938298), '.': np.float32(-3.1676342), ')': np.float32(-4.0035467), ']': np.float32(-4.676032), '  ': np.float32(-4.822523), ';': np.float32(-4.828734), ' (': np.float32(-5.1818156), '': np.float32(-5.485891)}, {'8': np.float32(-0.28877068), '9': np.float32(-1.8444738), '7': np.float32(-3.6920395), '1': np.float32(-3.9227524), '6': np.float32(-4.400029), '5': np.float32(-5.1092167), ' ': np.float32(-5.4198284), '\\n': np.float32(-5.8900433), '2': np.float32(-6.184655), '4': np.float32(-6.4175377)}, {'.': np.float32(-0.023853585), ',': np.float32(-4.288502), '\\n': np.float32(-6.112779), ' ': np.float32(-7.015248), ':': np.float32(-7.1900487), '0': np.float32(-7.974734), '.,': np.float32(-8.000046), ')': np.float32(-8.13658), '+': np.float32(-8.56815), '1': np.float32(-8.719374)}, {'1': np.float32(-1.5810726), '2': np.float32(-1.7263172), '0': np.float32(-1.9264953), '3': np.float32(-1.9690464), '4': np.float32(-2.2804735), '5': np.float32(-2.535052), '6': np.float32(-2.9768655), '7': np.float32(-3.239848), '9': np.float32(-3.4828994), '8': np.float32(-3.622794)}, {',': np.float32(-0.21892065), '\\n': np.float32(-3.216323), ' ': np.float32(-3.3549204), '.': np.float32(-4.1846952), ')': np.float32(-4.485257), ']': np.float32(-5.248848), ' (': np.float32(-5.260604), ';': np.float32(-5.5592813), '': np.float32(-5.575798), '  ': np.float32(-5.8729534)}, {'1': np.float32(-0.08255476), '9': np.float32(-3.6462135), '8': np.float32(-4.1277857), '7': np.float32(-4.9267883), '2': np.float32(-5.5037985), ' ': np.float32(-5.5255795), '\\n': np.float32(-5.7717648), '6': np.float32(-5.86522), '0': np.float32(-5.887374), '5': np.float32(-6.390897)}, {'1': np.float32(-0.6781656), '0': np.float32(-0.9005224), '2': np.float32(-2.6505034), '3': np.float32(-5.017185), '.': np.float32(-5.8104115), '4': np.float32(-6.172659), '5': np.float32(-6.4236727), '6': np.float32(-7.3454723), '7': np.float32(-7.8201447), '9': np.float32(-7.8382845)}, {'.': np.float32(-0.05614332), ',': np.float32(-3.6428304), '\\n': np.float32(-4.9482837), ' ': np.float32(-5.8048577), ':': np.float32(-6.4337506), '.,': np.float32(-6.631497), ')': np.float32(-6.757296), ']': np.float32(-7.2026057), '  ': np.float32(-7.495945), '-': np.float32(-7.569258)}, {'0': np.float32(-1.952656), '1': np.float32(-1.9854414), '2': np.float32(-2.0948994), '5': np.float32(-2.1723568), '3': np.float32(-2.2629263), '4': np.float32(-2.3562682), '6': np.float32(-2.5068934), '7': np.float32(-2.5223277), '8': np.float32(-2.7796824), '9': np.float32(-2.9998534)}, {',': np.float32(-0.30217975), '\\n': np.float32(-2.6517787), ' ': np.float32(-3.076795), '.': np.float32(-3.5256104), ')': np.float32(-4.66904), ']': np.float32(-5.2431493), '': np.float32(-5.325566), ';': np.float32(-5.4618297), '  ': np.float32(-5.4956946), ' (': np.float32(-5.6940846)}, {'1': np.float32(-0.04613561), '2': np.float32(-4.37518), '9': np.float32(-5.3844104), '\\n': np.float32(-5.6454988), ' ': np.float32(-5.7564645), '8': np.float32(-5.791163), '3': np.float32(-6.109158), '0': np.float32(-6.2829375), '7': np.float32(-6.3544965), '5': np.float32(-6.517999)}, {'3': np.float32(-0.68167955), '4': np.float32(-1.2023551), '2': np.float32(-2.4461677), '5': np.float32(-2.7245452), '1': np.float32(-4.195606), '6': np.float32(-4.7923336), '0': np.float32(-4.819417), '7': np.float32(-5.630558), '.': np.float32(-5.6893425), '8': np.float32(-6.6824102)}, {'.': np.float32(-0.022560485), ',': np.float32(-4.381861), '\\n': np.float32(-5.90091), ' ': np.float32(-7.2760606), ':': np.float32(-7.341247), ')': np.float32(-8.3291), ']': np.float32(-8.37343), '.,': np.float32(-8.497519), '-': np.float32(-8.614729), '': np.float32(-8.925919)}]}\n","### Running q8_0\n","\tresponse = 6.7,7.6,8.9,10.4,12.\n","\telapsed_time = {'tokens': ['6', '.', '7', ',', '7', '.', '6', ',', '8', '.', '9', ',', '1', '0', '.', '4', ',', '1', '2', '.'], 'text_offset': [20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39], 'token_logprobs': [np.float32(-0.25894302), np.float32(-0.0057795793), np.float32(-2.1742172), np.float32(-0.34144422), np.float32(-0.16035551), np.float32(-0.006598825), np.float32(-2.1169732), np.float32(-0.3088426), np.float32(-0.12519343), np.float32(-0.0070880386), np.float32(-2.8095891), np.float32(-0.2751682), np.float32(-1.0884311), np.float32(-0.09535594), np.float32(-0.0718088), np.float32(-2.306861), np.float32(-0.50426584), np.float32(-0.110834435), np.float32(-0.5808073), np.float32(-0.03789382)], 'top_logprobs': [{'6': np.float32(-0.25894302), '7': np.float32(-2.451263), '5': np.float32(-3.515657), '1': np.float32(-4.148648), '8': np.float32(-4.248689), '4': np.float32(-4.7750382), ' ': np.float32(-4.8746333), '\\n': np.float32(-5.050343), '2': np.float32(-5.0760646), '3': np.float32(-5.192096)}, {'.': np.float32(-0.0057795793), ',': np.float32(-5.89856), '\\n': np.float32(-7.415891), ':': np.float32(-8.466242), ' ': np.float32(-9.075601), ')': np.float32(-9.076256), ' (': np.float32(-9.128319), '-': np.float32(-9.26556), '(': np.float32(-9.33746), ';': np.float32(-9.690413)}, {'8': np.float32(-1.8337642), '9': np.float32(-1.9635926), '5': np.float32(-1.9834167), '6': np.float32(-2.124776), '7': np.float32(-2.1742172), '4': np.float32(-2.401598), '3': np.float32(-2.6129885), '1': np.float32(-2.7087107), '2': np.float32(-2.7234945), '0': np.float32(-3.4617662)}, {',': np.float32(-0.34144422), '\\n': np.float32(-2.5198166), '.': np.float32(-3.298097), ' ': np.float32(-3.4349387), ')': np.float32(-4.0786686), ']': np.float32(-5.152095), ' (': np.float32(-5.165574), ';': np.float32(-5.1896963), '/': np.float32(-5.287839), '  ': np.float32(-5.3503046)}, {'7': np.float32(-0.16035551), '8': np.float32(-2.7391243), '6': np.float32(-3.2590704), '5': np.float32(-4.914812), '9': np.float32(-4.969433), '1': np.float32(-5.068186), ' ': np.float32(-5.4540234), '\\n': np.float32(-5.8637686), '4': np.float32(-6.327485), '2': np.float32(-6.620552)}, {'.': np.float32(-0.006598825), ',': np.float32(-5.76444), '\\n': np.float32(-7.19072), ')': np.float32(-8.268014), ' ': np.float32(-8.604642), ':': np.float32(-8.670178), '-': np.float32(-8.88531), '(': np.float32(-9.343091), ' (': np.float32(-9.413751), ']': np.float32(-9.509557)}, {'7': np.float32(-1.9206927), '8': np.float32(-1.990782), '5': np.float32(-2.0763285), '6': np.float32(-2.1169732), '4': np.float32(-2.2645142), '9': np.float32(-2.292681), '3': np.float32(-2.4759877), '2': np.float32(-2.6476376), '1': np.float32(-2.7119715), '0': np.float32(-3.1485379)}, {',': np.float32(-0.3088426), '\\n': np.float32(-2.8187323), ' ': np.float32(-3.1394234), '.': np.float32(-3.8737545), ')': np.float32(-4.157483), ']': np.float32(-4.9933825), ';': np.float32(-5.1362677), ' (': np.float32(-5.1946936), '  ': np.float32(-5.6418157), '\\r': np.float32(-5.783226)}, {'8': np.float32(-0.12519343), '9': np.float32(-3.3134859), '7': np.float32(-3.5887773), '6': np.float32(-4.493183), '1': np.float32(-4.7206287), ' ': np.float32(-5.013713), '5': np.float32(-5.5890675), '\\n': np.float32(-5.590494), '2': np.float32(-6.602331), '4': np.float32(-6.7781086)}, {'.': np.float32(-0.0070880386), ',': np.float32(-5.4260116), '\\n': np.float32(-7.381209), ' ': np.float32(-8.847584), '.,': np.float32(-8.879929), ':': np.float32(-8.95106), ')': np.float32(-9.095562), '0': np.float32(-9.67454), '5': np.float32(-9.969625), '7': np.float32(-10.086169)}, {'5': np.float32(-1.3993261), '6': np.float32(-1.8389261), '7': np.float32(-1.8666484), '4': np.float32(-2.2331598), '8': np.float32(-2.4717395), '3': np.float32(-2.5892246), '9': np.float32(-2.8095891), '2': np.float32(-3.0090053), '1': np.float32(-3.2703474), '0': np.float32(-3.709006)}, {',': np.float32(-0.2751682), '\\n': np.float32(-2.9574573), ' ': np.float32(-3.0944984), '.': np.float32(-3.7472408), ')': np.float32(-4.21587), ']': np.float32(-4.6767855), '  ': np.float32(-5.463723), '\\r': np.float32(-5.4910107), ';': np.float32(-5.7489614), ' (': np.float32(-5.8216295)}, {'9': np.float32(-0.5564354), '1': np.float32(-1.0884311), '8': np.float32(-3.3443143), '7': np.float32(-3.9098032), '6': np.float32(-4.701988), ' ': np.float32(-5.6611958), '5': np.float32(-5.752928), '\\n': np.float32(-5.8536406), '2': np.float32(-5.93412), '0': np.float32(-6.068741)}, {'0': np.float32(-0.09535594), '1': np.float32(-2.814355), '2': np.float32(-4.2418385), '.': np.float32(-4.9636517), '3': np.float32(-5.8606586), '5': np.float32(-6.2751255), '4': np.float32(-6.5618124), '6': np.float32(-7.198819), '9': np.float32(-7.23827), '7': np.float32(-7.361124)}, {'.': np.float32(-0.0718088), ',': np.float32(-4.019251), '0': np.float32(-4.7138557), '\\n': np.float32(-4.8573027), ' ': np.float32(-5.0059576), ')': np.float32(-6.0292463), ':': np.float32(-6.0670605), '  ': np.float32(-6.3631907), '1': np.float32(-6.768777), ']': np.float32(-7.3399825)}, {'0': np.float32(-1.5437325), '1': np.float32(-1.826681), '2': np.float32(-2.0207863), '3': np.float32(-2.1597404), '4': np.float32(-2.306861), '5': np.float32(-2.3845005), '6': np.float32(-2.8228168), '7': np.float32(-2.9717903), '8': np.float32(-3.267621), '9': np.float32(-3.491826)}, {',': np.float32(-0.50426584), ' ': np.float32(-2.3347816), '\\n': np.float32(-2.4749298), '.': np.float32(-3.439993), ')': np.float32(-3.7241764), ' (': np.float32(-4.70109), ']': np.float32(-4.735214), ';': np.float32(-5.112894), '': np.float32(-5.4196506), '\"': np.float32(-5.6410866)}, {'1': np.float32(-0.110834435), '9': np.float32(-3.750521), '2': np.float32(-4.000365), '8': np.float32(-4.487987), '7': np.float32(-5.1254697), '0': np.float32(-5.1982684), '\\n': np.float32(-5.263313), '6': np.float32(-5.366506), ' ': np.float32(-5.544479), '5': np.float32(-5.6029096)}, {'2': np.float32(-0.5808073), '1': np.float32(-1.1937376), '3': np.float32(-2.658832), '0': np.float32(-3.187264), '4': np.float32(-4.535298), '5': np.float32(-5.0283246), '.': np.float32(-5.481704), '6': np.float32(-6.451225), '7': np.float32(-6.923182), '8': np.float32(-7.3887563)}, {'.': np.float32(-0.03789382), ',': np.float32(-3.8787103), '\\n': np.float32(-5.5151057), ' ': np.float32(-6.6268954), ':': np.float32(-7.178901), ')': np.float32(-7.3973727), '.,': np.float32(-7.824158), ']': np.float32(-7.8326225), '-': np.float32(-7.9419684), '+': np.float32(-8.018802)}]}\n"]}]},{"cell_type":"code","source":["results['q8_0']['logprobs_data']"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"T0v9uaNlc9vX","executionInfo":{"status":"ok","timestamp":1751820273784,"user_tz":240,"elapsed":24,"user":{"displayName":"Marcelo Labre","userId":"11402290200752388716"}},"outputId":"c906cbc9-0e01-4b38-f8b5-92121e664563"},"execution_count":93,"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'tokens': ['6',\n","  ',',\n","  '7',\n","  ',',\n","  '8',\n","  ',',\n","  '9',\n","  ',',\n","  '1',\n","  '0',\n","  '\\n',\n","  '\\n',\n","  '\\n',\n","  '\\n',\n","  '\\n',\n","  '\\n',\n","  '\\n',\n","  '\\n',\n","  '\\n',\n","  '\\n'],\n"," 'text_offset': [10,\n","  11,\n","  12,\n","  13,\n","  14,\n","  15,\n","  16,\n","  17,\n","  18,\n","  19,\n","  20,\n","  21,\n","  22,\n","  23,\n","  24,\n","  25,\n","  26,\n","  27,\n","  28,\n","  29],\n"," 'token_logprobs': [np.float32(-0.039400734),\n","  np.float32(-0.22049828),\n","  np.float32(-0.017327122),\n","  np.float32(-0.053606026),\n","  np.float32(-0.004326626),\n","  np.float32(-0.05266202),\n","  np.float32(-0.003440891),\n","  np.float32(-0.1316747),\n","  np.float32(-0.24304207),\n","  np.float32(-0.003549349),\n","  np.float32(-1.8983413),\n","  np.float32(-1.7544292),\n","  np.float32(-4.7411046),\n","  np.float32(-3.024836),\n","  np.float32(-2.5478287),\n","  np.float32(-2.4488103),\n","  np.float32(-1.4894649),\n","  np.float32(-0.7896662),\n","  np.float32(-0.56893784),\n","  np.float32(-0.36428946)],\n"," 'top_logprobs': [{'6': np.float32(-0.039400734),\n","   ' ': np.float32(-5.1428456),\n","   '7': np.float32(-5.402578),\n","   '\\n': np.float32(-5.5278707),\n","   '1': np.float32(-5.6789145),\n","   ' or': np.float32(-6.4467597),\n","   '8': np.float32(-6.6105638),\n","   ' etc': np.float32(-7.0784554),\n","   '9': np.float32(-7.4321294),\n","   ' and': np.float32(-7.520734)},\n","  {',': np.float32(-0.22049828),\n","   '\\n': np.float32(-3.3176806),\n","   ' ': np.float32(-3.8120072),\n","   '.': np.float32(-4.372164),\n","   ')': np.float32(-4.803979),\n","   ' (': np.float32(-4.926985),\n","   ' |': np.float32(-5.418931),\n","   ']': np.float32(-5.419915),\n","   '|': np.float32(-5.609479),\n","   ':': np.float32(-5.6310062)},\n","  {'7': np.float32(-0.017327122),\n","   '8': np.float32(-5.848632),\n","   ' ': np.float32(-5.98967),\n","   '\\n': np.float32(-6.1592207),\n","   '1': np.float32(-6.748766),\n","   '9': np.float32(-7.256734),\n","   ' or': np.float32(-7.6680317),\n","   '6': np.float32(-8.0953455),\n","   '...': np.float32(-8.225471),\n","   '  ': np.float32(-8.237595)},\n","  {',': np.float32(-0.053606026),\n","   '\\n': np.float32(-4.4010324),\n","   ' ': np.float32(-5.006323),\n","   '.': np.float32(-5.8670673),\n","   ')': np.float32(-6.2889338),\n","   '|': np.float32(-6.3474684),\n","   ' (': np.float32(-6.4676495),\n","   ' or': np.float32(-6.5599766),\n","   ' |': np.float32(-6.784598),\n","   ']': np.float32(-6.8173013)},\n","  {'8': np.float32(-0.004326626),\n","   ' ': np.float32(-6.96838),\n","   '\\n': np.float32(-7.6377535),\n","   '9': np.float32(-7.860074),\n","   ' or': np.float32(-8.3399935),\n","   '1': np.float32(-8.511912),\n","   '  ': np.float32(-9.055935),\n","   ' etc': np.float32(-9.349964),\n","   '7': np.float32(-9.57958),\n","   '...': np.float32(-9.646569)},\n","  {',': np.float32(-0.05266202),\n","   '\\n': np.float32(-4.299345),\n","   ' ': np.float32(-5.139304),\n","   '.': np.float32(-5.864012),\n","   '|': np.float32(-6.2174397),\n","   ')': np.float32(-6.322212),\n","   ' (': np.float32(-6.6315966),\n","   ' or': np.float32(-6.8171625),\n","   ':': np.float32(-6.89087),\n","   '': np.float32(-7.0901756)},\n","  {'9': np.float32(-0.003440891),\n","   ' ': np.float32(-7.2129374),\n","   '1': np.float32(-7.41498),\n","   '\\n': np.float32(-7.7854595),\n","   ' or': np.float32(-8.275633),\n","   '0': np.float32(-9.524151),\n","   '8': np.float32(-9.692198),\n","   '<': np.float32(-10.167011),\n","   ' etc': np.float32(-10.328886),\n","   '  ': np.float32(-10.33885)},\n","  {',': np.float32(-0.1316747),\n","   '\\n': np.float32(-3.5694294),\n","   ' ': np.float32(-4.356039),\n","   '.': np.float32(-4.7465696),\n","   ')': np.float32(-5.160141),\n","   ' (': np.float32(-5.427329),\n","   ']': np.float32(-5.882335),\n","   '|': np.float32(-6.037788),\n","   ' |': np.float32(-6.0799365),\n","   ' or': np.float32(-6.1211395)},\n","  {'1': np.float32(-0.24304207),\n","   '0': np.float32(-1.6656953),\n","   '\\n': np.float32(-5.5091705),\n","   ' ': np.float32(-5.767356),\n","   'A': np.float32(-6.412921),\n","   ' or': np.float32(-6.951852),\n","   ' A': np.float32(-7.1078625),\n","   '<': np.float32(-7.2600555),\n","   'a': np.float32(-7.3567724),\n","   '2': np.float32(-7.5869145)},\n","  {'0': np.float32(-0.003549349),\n","   '1': np.float32(-6.8583364),\n","   ',': np.float32(-6.975527),\n","   '\\n': np.float32(-8.375431),\n","   '2': np.float32(-8.4128),\n","   ' ': np.float32(-8.642626),\n","   '<': np.float32(-9.287398),\n","   '.': np.float32(-9.639219),\n","   '5': np.float32(-9.860963),\n","   '3': np.float32(-9.959633)},\n","  {',': np.float32(-0.8389064),\n","   '\\n': np.float32(-1.8983413),\n","   ' ': np.float32(-2.4966145),\n","   '.': np.float32(-3.3665848),\n","   ')': np.float32(-3.8639402),\n","   ' (': np.float32(-4.000184),\n","   '  ': np.float32(-4.4696593),\n","   ' |': np.float32(-4.8259544),\n","   ']': np.float32(-4.8758163),\n","   '': np.float32(-4.900792)},\n","  {'-': np.float32(-1.2788032),\n","   '\\n': np.float32(-1.7544292),\n","   '<': np.float32(-1.9950317),\n","   '1': np.float32(-3.061232),\n","   '2': np.float32(-3.4772983),\n","   '3': np.float32(-4.0877643),\n","   '   ': np.float32(-4.135432),\n","   '```': np.float32(-4.271401),\n","   '[': np.float32(-4.7239823),\n","   '|': np.float32(-4.7729096)},\n","  {'Ex': np.float32(-2.4313607),\n","   'The': np.float32(-3.2205148),\n","   'Example': np.float32(-3.3714604),\n","   '1': np.float32(-3.6336865),\n","   'S': np.float32(-3.771422),\n","   'In': np.float32(-3.7779622),\n","   'I': np.float32(-3.8533568),\n","   '5': np.float32(-3.911265),\n","   '```': np.float32(-4.0163445),\n","   'This': np.float32(-4.1312833),\n","   '\\n': np.float32(-4.7411046)},\n","  {'A': np.float32(-1.1765915),\n","   'Ex': np.float32(-2.9967752),\n","   '\\n': np.float32(-3.024836),\n","   '*': np.float32(-3.0437846),\n","   'B': np.float32(-3.647551),\n","   '```': np.float32(-3.7282882),\n","   'I': np.float32(-3.7860837),\n","   '': np.float32(-4.018884),\n","   'Example': np.float32(-4.2500434),\n","   '2': np.float32(-4.343552)},\n","  {'\\n': np.float32(-2.5478287),\n","   '2': np.float32(-2.8500853),\n","   'Ex': np.float32(-3.1576462),\n","   'A': np.float32(-3.2674932),\n","   '3': np.float32(-3.3853607),\n","   '```': np.float32(-3.4328098),\n","   'Example': np.float32(-3.4734306),\n","   '4': np.float32(-3.5216494),\n","   '5': np.float32(-3.5821915),\n","   'B': np.float32(-3.6694908)},\n","  {'\\n': np.float32(-2.4488103),\n","   '```': np.float32(-2.619312),\n","   'A': np.float32(-3.216993),\n","   'Ex': np.float32(-3.33135),\n","   'B': np.float32(-3.4130971),\n","   '2': np.float32(-3.429732),\n","   'I': np.float32(-3.4923913),\n","   '<': np.float32(-3.568155),\n","   'In': np.float32(-3.860216),\n","   'The': np.float32(-3.9367874)},\n","  {'\\n': np.float32(-1.4894649),\n","   '<': np.float32(-2.356697),\n","   'A': np.float32(-2.5205393),\n","   '': np.float32(-3.095952),\n","   '```': np.float32(-3.1229296),\n","   'B': np.float32(-3.278491),\n","   'I': np.float32(-3.521062),\n","   'Ex': np.float32(-3.6807137),\n","   'In': np.float32(-4.196913),\n","   '2': np.float32(-4.2238245)},\n","  {'\\n': np.float32(-0.7896662),\n","   '<': np.float32(-1.9318085),\n","   '': np.float32(-2.8765268),\n","   '```': np.float32(-3.251814),\n","   'A': np.float32(-3.9620895),\n","   'B': np.float32(-4.3854513),\n","   'I': np.float32(-4.5692434),\n","   'In': np.float32(-5.0097),\n","   'The': np.float32(-5.0532484),\n","   'S': np.float32(-5.069766)},\n","  {'\\n': np.float32(-0.56893784),\n","   '<': np.float32(-2.193252),\n","   '': np.float32(-2.818398),\n","   '```': np.float32(-3.269586),\n","   'A': np.float32(-4.8078465),\n","   'I': np.float32(-4.9629016),\n","   'B': np.float32(-5.00048),\n","   'In': np.float32(-5.175936),\n","   'The': np.float32(-5.2458405),\n","   'Ex': np.float32(-5.3359656)},\n","  {'\\n': np.float32(-0.36428946),\n","   '<': np.float32(-2.47489),\n","   '': np.float32(-2.970379),\n","   '```': np.float32(-3.561579),\n","   'A': np.float32(-4.501831),\n","   'I': np.float32(-5.523942),\n","   'B': np.float32(-5.542755),\n","   'In': np.float32(-5.7461014),\n","   'The': np.float32(-5.822157),\n","   'Ex': np.float32(-5.943754)}]}"]},"metadata":{},"execution_count":93}]},{"cell_type":"code","source":[],"metadata":{"id":"DW_sCz5cavrA"},"execution_count":null,"outputs":[]},{"cell_type":"code","source":[],"metadata":{"id":"Odmzw4T8avtt"},"execution_count":null,"outputs":[]}]}