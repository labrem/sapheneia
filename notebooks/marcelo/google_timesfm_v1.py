# -*- coding: utf-8 -*-
"""google_timesfm_v1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1MgOre2e8R0kp6L89Qa1Q4CF1m6WYbEC2

# setup
"""

import os
import torch
import time
import numpy as np
import pandas as pd
import math
import re
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
from tqdm import tqdm
from sklearn.preprocessing import StandardScaler

# Check if a CUDA-enabled GPU is available
if torch.cuda.is_available():
    print("GPU detected. Installing the CUDA-enabled JAX ...")
    device_backend = "gpu"
    !pip install -U "jax[cuda12_pip]" -f https://storage.googleapis.com/jax-releases/jax_cuda_releases.html
else:
    print("No GPU detected. Installing the CPU-only JAX ...")
    device_backend = "cpu"
    !pip install --upgrade "jax[cpu]"
import jax
print(jax.devices())

# Install the PyTorch version of the timesfm library
!pip install --upgrade timesfm[torch]
import timesfm

# Mount Google Drive
from google.colab import drive
drive.mount('/content/drive/',force_remount=True)

path = '/content/drive/MyDrive/dev/'

if torch.cuda.is_available():
    print(f"CUDA version: {torch.version.cuda}")

if torch.cuda.is_available():
    !nvcc --version

if torch.cuda.is_available():
    !nvidia-smi

"""# data"""

# Load World M2 (wm2) and Bitcoin (btc) data
data_path = path+"sapheneia/github/sapheneia/data/"
btc  = pd.read_csv(data_path+"btc.csv")
wm2 = pd.read_csv(data_path+"wm2.csv")

# Define the horizon for computation of log returns in each time-series
# Given the use case of BTC and WM2 and that WM2 is given in weeks, the number of periods for horizon will be in weeks
logreturn_horizon = 1

# Convert 'Date' column in btc to datetime objects
btc['Date'] = pd.to_datetime(btc['Date'])

# Convert 'observation_date' column in wm2 to datetime objects
wm2['observation_date'] = pd.to_datetime(wm2['observation_date'])

# Merge the two dataframes on the date columns
data = pd.merge(btc, wm2, left_on='Date', right_on='observation_date', how='inner')
data = data[['Date','Close','WM2NS']]
data = data.rename(columns={'Date': 'date','Close': 'btc','WM2NS': 'wm2'})

# Display the first few rows of the merged dataframe
display(data.head())

# prompt: Using dataframe data: Compute a column for each btc and wm2 with the changes from one period to another. Change here is defined as log(x_2,x_1) where x_2 is the value of the column in a period ahead of the value x_1, where the number of periods between x_2 and x_1 is defined by a parameter horizon. Start with the first x_1 as the first period in the dataframe.

# Calculate the log change for 'btc'
data['btc_logreturn'] = np.log(data['btc'] / data['btc'].shift(logreturn_horizon))

# Calculate the log change for 'wm2'
data['wm2_logreturn'] = np.log(data['wm2'] / data['wm2'].shift(logreturn_horizon))

# prompt: Using dataframe data: Now given btc_change and wm2_change, first clean up rows NaN.

# Clean up rows with NaN values in 'btc_change' or 'wm2_change'
data.dropna(subset=['btc_logreturn', 'wm2_logreturn'], inplace=True)

# Round btc_change and wm2_change to 4 decimal places
data['btc_logreturn'] = data['btc_logreturn'].round(4)
data['wm2_logreturn'] = data['wm2_logreturn'].round(4)

# Sort by date to ensure calculations are in chronological order
data.sort_values(by='date', inplace=True)

# Display the first few rows of the merged dataframe
display(data.head())

series_a = data['wm2'].tolist()
print(series_a)

series_b = data['btc'].tolist()
print(series_b)

len(data)

"""# model"""

# Define parameters for the synthetic data
# CONTEXT_LEN = 512
# HORIZON_LEN = 128
CONTEXT = 64
HORIZON = 20

# Load model from local drive
model_path = path+'.models/google-timesfm-2.0-500m-pytorch/'

# Load model from HuggingFace
#model_path = ''

if model_path=='':
    checkpoint = timesfm.TimesFmCheckpoint(huggingface_repo_id="google/timesfm-2.0-500m-pytorch")
else:
    model_file = os.path.join(model_path, 'torch_model.ckpt')
    checkpoint = timesfm.TimesFmCheckpoint(path=model_file)
checkpoint

# Initialize the TimesFm model
tfm = timesfm.TimesFm(
    hparams=timesfm.TimesFmHparams(
        backend=device_backend,  # Use "gpu" if a GPU runtime is selected
        context_len=CONTEXT,
        horizon_len=HORIZON,
        # The following parameters are fixed for the 2.0-500m model
        input_patch_len=32,
        output_patch_len=128,
        num_layers=50,
        model_dims=1280,
        use_positional_embedding=False,
    ),
    checkpoint=checkpoint,
)

"""# experiments

## (1) basic with synthetic data (sine + noise)
"""

# Create a synthetic time-series: a sine wave with noise
# context_data = np.sin(np.linspace(0, 40, CONTEXT_LEN)) + np.random.randn(CONTEXT_LEN) * 0.1
time = np.arange(CONTEXT_LEN)
context_data = np.sin(time * 0.1) + np.sin(time * 0.25) + np.random.randn(len(time)) * 0.5

# The input for the forecast function must be a list of numpy arrays
forecast_input = [context_data]

# Run the forecast
# The `freq` input is a list of integers corresponding to the input series.
# 0 = high frequency (e.g., daily), 1 = medium (e.g., weekly), 2 = low (e.g., yearly)
point_forecast, experimental_quantile_forecast = tfm.forecast(
    forecast_input,
    freq=[0],
)

import matplotlib.pyplot as plt
import numpy as np

# Assume CONTEXT_LEN and HORIZON_LEN are defined in your environment
# Assume context_data and point_forecast already exist from your previous steps

plt.figure(figsize=(12, 6))

# Plot the historical data that the model used as input
plt.plot(np.arange(CONTEXT_LEN), context_data, label="Historical Context")

# Plot the forecasted data
# The forecast starts at the end of the context
forecast_horizon = np.arange(CONTEXT_LEN, CONTEXT_LEN + HORIZON_LEN)

# By using point_forecast[0], we select the 1D array from the 2D batch, fixing the error.
plt.plot(forecast_horizon, point_forecast[0], color='red', label="TimesFM Forecast")

# --- Formatting ---
plt.title("TimesFM Forecast")
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.show()

experimental_quantile_forecast, experimental_quantile_forecast.shape

import matplotlib.pyplot as plt
import numpy as np

# Assume CONTEXT_LEN and HORIZON_LEN are defined in your environment
# Assume context_data, point_forecast, and experimental_quantile_forecast already exist

plt.figure(figsize=(15, 7))

# 1. Plot the historical data that the model used as input
plt.plot(np.arange(CONTEXT_LEN), context_data, 'b-', label="Historical Context")

# Define the x-axis for the forecast period
forecast_horizon = np.arange(CONTEXT_LEN, CONTEXT_LEN + HORIZON_LEN)

# 2. Extract and plot the quantile forecasts to create the "cloud"
# First, get the forecast batch (index 0) from the quantile data
quantile_forecasts_1d = experimental_quantile_forecast[0]

# Plot the 80% confidence interval (between 0.1 and 0.9 quantiles)
plt.fill_between(
    forecast_horizon,
    quantile_forecasts_1d[:, 0], # 0.1 quantile at index 0
    quantile_forecasts_1d[:, 8], # 0.9 quantile at index 8
    color='gray',
    alpha=0.3,
    label='80% Confidence Interval'
)

# Plot the 40% confidence interval (between 0.3 and 0.7 quantiles)
plt.fill_between(
    forecast_horizon,
    quantile_forecasts_1d[:, 2], # 0.3 quantile at index 2
    quantile_forecasts_1d[:, 6], # 0.7 quantile at index 6
    color='gray',
    alpha=0.5,
    label='40% Confidence Interval'
)

# 3. Plot the main point forecast
# We use point_forecast[0] to select the 1D array from the 2D batch
plt.plot(forecast_horizon, point_forecast[0], 'r-', label="Point Forecast")


# --- Formatting ---
plt.title("TimesFM Probabilistic Forecast", fontsize=16)
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.autoscale(axis='x', tight=True)
plt.show()

"""## (2) btc, normalized log-returns, no covariate

### code
"""

def sigmoid(x):
    return 1 / (1 + np.exp(-x))

def inverse_sigmoid(y):
    epsilon = 1e-9
    y = np.clip(y, epsilon, 1 - epsilon)
    return np.log(y / (1 - y))

def fit_normalizer(data):

    data_reshaped = data.reshape(-1, 1)

    # Fit a StandardScaler to the raw data
    normalizer = StandardScaler()
    normalizer.fit(data_reshaped)

    return normalizer

def normalize_data(data, normalizer, sigmoid=False):

    data_reshaped = data.reshape(-1, 1)

    # Standardize the data (mean=0, std=1)
    x = normalizer.transform(data_reshaped)

    if sigmoid:
        x = sigmoid(x).flatten()

    return x.reshape(-1)

def denormalize_data(data, normalizer, sigmoid=False):

    if sigmoid:
        x = inverse_sigmoid(data)
    else:
        x = data

    return normalizer.inverse_transform(x.reshape(1, -1)).reshape(-1)

def compute_logreturns(data, lookback=1):

    logdata = np.log(data)

    return logdata[lookback:] - logdata[:-lookback]

def reverse_logreturns(initial, logreturns):

    return np.exp(np.cumsum(logreturns))*initial

"""### experiment"""

lookback = 1

context = data['btc'][-(lookback+CONTEXT):].to_numpy()
horizon = context[-HORIZON:]
context,horizon

x_log = compute_logreturns(context[:-HORIZON], lookback=lookback)
x_log

normalizer = fit_normalizer(x_log)
normalizer.mean_, normalizer.scale_

z_log = normalize_data(x_log, normalizer, sigmoid=False)
z_log

# Run the forecast
# The `freq` input is a list of integers corresponding to the input series.
# 0 = high frequency (e.g., daily), 1 = medium (e.g., weekly), 2 = low (e.g., yearly)
forecast_input = [z_log]
point_forecast, experimental_quantile_forecast = tfm.forecast(
    forecast_input,
    freq=[1],
)
point_forecast[0]

f_log = denormalize_data(data=point_forecast[0], normalizer=normalizer)
f_log

forecast = reverse_logreturns(initial=context[CONTEXT-HORIZON], logreturns=f_log)
forecast,len(forecast)

n = len(experimental_quantile_forecast[0])
quantiles = []
for q_forecast in experimental_quantile_forecast[0]:
    q_log = denormalize_data(data=q_forecast, normalizer=normalizer)
    q = reverse_logreturns(initial=context[CONTEXT-HORIZON], logreturns=q_log)
    quantiles.append(q)
quantiles = np.array(quantiles)

plt.figure(figsize=(15, 7))

# 1. Plot the historical data that the model used as input
plt.plot(np.arange(CONTEXT), context[1:], 'b-', label="Historical Context")

# Define the x-axis for the forecast period
forecast_horizon = np.arange(CONTEXT-HORIZON, CONTEXT)

# 2. Extract and plot the quantile forecasts to create the "cloud"
# First, get the forecast batch (index 0) from the quantile data
quantile_forecasts_1d = quantiles

# Plot the 80% confidence interval (between 0.1 and 0.9 quantiles)
plt.fill_between(
    forecast_horizon,
    quantile_forecasts_1d[:, 0], # 0.1 quantile at index 0
    quantile_forecasts_1d[:, 8], # 0.9 quantile at index 8
    color='gray',
    alpha=0.3,
    label='80% Confidence Interval'
)

# Plot the 40% confidence interval (between 0.3 and 0.7 quantiles)
plt.fill_between(
    forecast_horizon,
    quantile_forecasts_1d[:, 2], # 0.3 quantile at index 2
    quantile_forecasts_1d[:, 6], # 0.7 quantile at index 6
    color='gray',
    alpha=0.5,
    label='40% Confidence Interval'
)

# 3. Plot the main point forecast
# We use point_forecast[0] to select the 1D array from the 2D batch
plt.plot(forecast_horizon, forecast, 'r-', label="Point Forecast")


# --- Formatting ---
plt.title("TimesFM Probabilistic Forecast", fontsize=16)
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.autoscale(axis='x', tight=True)
plt.show()

"""## (2) btc, no normalization or log-returns, no covariate

### experiment
"""

lookback = 1

context = data['btc'][-(lookback+CONTEXT):].to_numpy()
horizon = context[-HORIZON:]
context,context[:-HORIZON],horizon

# Run the forecast
# The `freq` input is a list of integers corresponding to the input series.
# 0 = high frequency (e.g., daily), 1 = medium (e.g., weekly), 2 = low (e.g., yearly)
forecast_input = [context[:-HORIZON]]
point_forecast, experimental_quantile_forecast = tfm.forecast(
    forecast_input,
    freq=[1],
)
point_forecast[0]

forecast = point_forecast[0]
quantiles = experimental_quantile_forecast[0]

forecast_horizon,np.arange(CONTEXT-HORIZON-1, CONTEXT),context[:-HORIZON][-1],context[-HORIZON-1],CONTEXT-HORIZON-1

np.arange(CONTEXT)

plt.figure(figsize=(15, 7))

# 1. Plot the historical data that the model used as input
plt.plot(np.arange(CONTEXT), context[1:], 'b-', label="Historical Context")

# Define the x-axis for the forecast period
forecast_horizon = np.arange(CONTEXT-HORIZON, CONTEXT)

# 2. Extract and plot the quantile forecasts to create the "cloud"
# First, get the forecast batch (index 0) from the quantile data
quantile_forecasts_1d = quantiles

# Plot the 80% confidence interval (between 0.1 and 0.9 quantiles)
plt.fill_between(
    forecast_horizon,
    quantile_forecasts_1d[:, 0], # 0.1 quantile at index 0
    quantile_forecasts_1d[:, 8], # 0.9 quantile at index 8
    color='gray',
    alpha=0.3,
    label='80% Confidence Interval'
)

# Plot the 40% confidence interval (between 0.3 and 0.7 quantiles)
plt.fill_between(
    forecast_horizon,
    quantile_forecasts_1d[:, 2], # 0.3 quantile at index 2
    quantile_forecasts_1d[:, 6], # 0.7 quantile at index 6
    color='gray',
    alpha=0.5,
    label='40% Confidence Interval'
)

# 3. Plot the main point forecast
# We use point_forecast[0] to select the 1D array from the 2D batch
x = [CONTEXT-HORIZON-1]+forecast_horizon.tolist()
y = [context[:-HORIZON][-1]]+forecast.tolist()

plt.plot(x, y, 'r-', label="Point Forecast")

# --- Formatting ---
plt.title("TimesFM Probabilistic Forecast", fontsize=16)
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.autoscale(axis='x', tight=True)
plt.show()

"""## (3) btc with wm2 covariate

### code
"""





"""### experiment"""

from timesfm import TimesFm

# Define your model's context and horizon lengths
model_context_len = 512
model_horizon_len = 64
num_covariates = 1 # We are using one covariate

# Isolate the input data
prediction_input_df = df.iloc[-model_context_len:]

# 1. Create the target series context
# Shape must be (batch_size, context_len) -> (1, 512)
context_ts = prediction_input_df['btc_price'].values.reshape(1, -1)

# 2. Create the covariate context
# Shape must be (batch_size, context_len, num_covariates) -> (1, 512, 1)
context_covariates = prediction_input_df['wm2'].values.reshape(1, model_context_len, num_covariates)

# 3. Create placeholder for future covariates
# Shape must be (batch_size, horizon_len, num_covariates) -> (1, 64, 1)
future_covariates = np.zeros((1, model_horizon_len, num_covariates))

# Load the model, making sure to specify num_covariates
tfm = TimesFm(
    context_len=model_context_len,
    horizon_len=model_horizon_len,
    num_covariates=num_covariates, # Crucial step!
    # ... other parameters
)
tfm.load_from_checkpoint(repo_id="google/timesfm-1.0-200m")

# Run the forecast using the specific method for covariates
point_forecasts, _ = tfm.forecast_with_covariates(
    context_ts,
    context_covariates,
    future_covariates,
)

num_covariates = 1

btc_context = data['btc'][-(CONTEXT+HORIZON):-HORIZON].values.reshape(1, -1)
btc_future = data['btc'][-HORIZON:].values.reshape(1, -1)
btc_context,btc_future

wm2_context = data['wm2'][-(CONTEXT+HORIZON):-HORIZON].values.reshape(1, CONTEXT, num_covariates)
wm2_future = np.zeros((1, HORIZON, num_covariates))
# wm2_future = data['wm2'][-HORIZON:].values.reshape(1, HORIZON, num_covariates)
wm2 = np.concatenate([wm2_context, wm2_future], axis=1)
# wm2_context,wm2_future

btc_context = btc_context.astype('float32')
wm2_context = wm2_context.astype('float32')
wm2_future = wm2_future.astype('float32')
wm2 = wm2.astype('float32')

covariate = {
    'wm2_covariate': wm2_context.reshape(1, -1).tolist()
}

# The `freq` input is a list of integers corresponding to the input series.
# 0 = high frequency (e.g., daily), 1 = medium (e.g., weekly), 2 = low (e.g., yearly)
point_forecast, experimental_quantile_forecast = tfm.forecast_with_covariates(
    inputs=btc_context,
    dynamic_numerical_covariates=covariate,
    freq=[1],
)

forecast = point_forecast[0]
forecast

plt.figure(figsize=(15, 7))

# 1. Plot the historical data that the model used as input
plt.plot(np.arange(CONTEXT), context[1:], 'b-', label="Historical Context")

# Define the x-axis for the forecast period
forecast_horizon = np.arange(CONTEXT-HORIZON, CONTEXT)

# 2. Extract and plot the quantile forecasts to create the "cloud"
# First, get the forecast batch (index 0) from the quantile data
quantile_forecasts_1d = quantiles

# Plot the 80% confidence interval (between 0.1 and 0.9 quantiles)
plt.fill_between(
    forecast_horizon,
    quantile_forecasts_1d[:, 0], # 0.1 quantile at index 0
    quantile_forecasts_1d[:, 8], # 0.9 quantile at index 8
    color='gray',
    alpha=0.3,
    label='80% Confidence Interval'
)

# Plot the 40% confidence interval (between 0.3 and 0.7 quantiles)
plt.fill_between(
    forecast_horizon,
    quantile_forecasts_1d[:, 2], # 0.3 quantile at index 2
    quantile_forecasts_1d[:, 6], # 0.7 quantile at index 6
    color='gray',
    alpha=0.5,
    label='40% Confidence Interval'
)

# 3. Plot the main point forecast
# We use point_forecast[0] to select the 1D array from the 2D batch
x = [CONTEXT-HORIZON-1]+forecast_horizon.tolist()
y = [context[:-HORIZON][-1]]+forecast.tolist()

plt.plot(x, y, 'r-', label="Point Forecast")

# --- Formatting ---
plt.title("TimesFM Probabilistic Forecast", fontsize=16)
plt.xlabel("Time Step")
plt.ylabel("Value")
plt.legend()
plt.grid(True)
plt.autoscale(axis='x', tight=True)
plt.show()





"""## test code"""

lookback = 1
window = 50
horizon = 10

x = data['btc'][-(lookback+window):].to_numpy()
x_log = compute_logreturns(x, lookback=lookback)
x_log

y_log = x_log[:-horizon]
normalizer = fit_normalizer(y_log)

x_log,y_log

z_log = normalize_data(y_log, normalizer, sigmoid=False)
z_log

denormalize_data(data=z_log, normalizer=normalizer)

h_log = normalize_data(x_log[-horizon:], normalizer, sigmoid=False)
h_log

normalize_data(x_log, normalizer, sigmoid=False)

#reverse_logreturns(data=x, logreturns=x_log, lookback=lookback)

np.exp(x_log)*x[:-lookback]

reverse_logreturns(data=x, logreturns=x_log, lookback=lookback)



